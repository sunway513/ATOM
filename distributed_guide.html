

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ATOM Distributed Inference Guide &mdash; ATOM 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=9edc463e" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=01f34227"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="ATOM Compilation &amp; CUDA Graphs Guide" href="compilation_cudagraph_guide.html" />
    <link rel="prev" title="ATOM Scheduling &amp; KV Cache Guide" href="scheduling_kv_cache_guide.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #C00000" >

          
          
          <a href="index.html" class="icon icon-home">
            ATOM
              <img src="_static/atom_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#installation-methods">Installation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="installation.html#from-source">From Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="installation.html#docker-installation">Docker Installation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#environment-variables">Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#verification">Verification</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#serving-a-model">Serving a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#batch-inference">Batch Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#distributed-serving">Distributed Serving</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#api-server">API Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#performance-tips">Performance Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#next-steps">Next Steps</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture_guide.html">ATOM Architecture Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#system-overview">1. System Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#component-architecture">2. Component Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#request-lifecycle">3. Request Lifecycle</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#forward-context-pattern">4. Forward Context Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#multi-process-architecture">5. Multi-Process Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#sequence-lifecycle">6. Sequence Lifecycle</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="configuration_guide.html">ATOM Configuration Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#master-configuration-config">1. Master Configuration (<code class="docutils literal notranslate"><span class="pre">Config</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#compilation-configuration-compilationconfig">2. Compilation Configuration (<code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#compilation-levels-compilationlevel">2.1 Compilation Levels (<code class="docutils literal notranslate"><span class="pre">CompilationLevel</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#compilationconfig-fields">2.2 <code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code> Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#cuda-graph-mode-cudagraphmode">2.3 CUDA Graph Mode (<code class="docutils literal notranslate"><span class="pre">CUDAGraphMode</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#quantization-configuration-quantizationconfig">3. Quantization Configuration (<code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#quantizationconfig-fields">3.1 <code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code> Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#quanttype-values-from-aiter">3.2 <code class="docutils literal notranslate"><span class="pre">QuantType</span></code> Values (from AITER)</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#supported-quantization-dtypes">3.3 Supported Quantization Dtypes</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#auto-detection-from-huggingface-get-quant-config">3.4 Auto-Detection from HuggingFace (<code class="docutils literal notranslate"><span class="pre">get_quant_config</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#parallel-configuration-parallelconfig">4. Parallel Configuration (<code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#speculative-decoding-configuration-speculativeconfig">5. Speculative Decoding Configuration (<code class="docutils literal notranslate"><span class="pre">SpeculativeConfig</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#sampling-parameters-samplingparams">6. Sampling Parameters (<code class="docutils literal notranslate"><span class="pre">SamplingParams</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#cli-arguments-engineargs">7. CLI Arguments (<code class="docutils literal notranslate"><span class="pre">EngineArgs</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#environment-variables">8. Environment Variables</a><ul>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#variables-registered-in-atom-utils-envs-py">8.1 Variables Registered in <code class="docutils literal notranslate"><span class="pre">atom/utils/envs.py</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#additional-environment-variables-used-outside-envs-py">8.2 Additional Environment Variables (Used Outside <code class="docutils literal notranslate"><span class="pre">envs.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#decision-tree-choosing-a-compilation-level">9. Decision Tree – Choosing a Compilation Level</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_support_guide.html">ATOM Model Support Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#supported-model-architectures">1. Supported Model Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#model-architecture-details">2. Model Architecture Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#qwen3-qwen3forcausallm">Qwen3 (<code class="docutils literal notranslate"><span class="pre">Qwen3ForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#qwen3-moe-qwen3moeforcausallm">Qwen3-MoE (<code class="docutils literal notranslate"><span class="pre">Qwen3MoeForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#llama-llamaforcausallm">Llama (<code class="docutils literal notranslate"><span class="pre">LlamaForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#mixtral-mixtralforcausallm">Mixtral (<code class="docutils literal notranslate"><span class="pre">MixtralForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#deepseek-v2-v3-deepseekv2forcausallm">DeepSeek V2/V3 (<code class="docutils literal notranslate"><span class="pre">DeepseekV2ForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#deepseek-mtp-deepseekmtp">DeepSeek MTP (<code class="docutils literal notranslate"><span class="pre">DeepSeekMTP</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#gpt-oss-gptossforcausallm">GPT-OSS (<code class="docutils literal notranslate"><span class="pre">GptOssForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#glm4-moe-glm4moeforcausallm">GLM4-MoE (<code class="docutils literal notranslate"><span class="pre">Glm4MoeForCausalLM</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#weight-loading">3. Weight Loading</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#function-signature">Function Signature</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#loading-flow">Loading Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#layers-beyond-num-hidden-layers">Layers Beyond <code class="docutils literal notranslate"><span class="pre">num_hidden_layers</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#adding-a-new-model">4. Adding a New Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#step-1-create-the-model-file">Step 1: Create the Model File</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#step-2-implement-layer-classes">Step 2: Implement Layer Classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#step-3-implement-the-model-and-causallm-classes">Step 3: Implement the Model and CausalLM Classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#step-4-register-the-model">Step 4: Register the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#step-5-handle-weight-loading">Step 5: Handle Weight Loading</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#model-specific-optimizations">5. Model-Specific Optimizations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#llama-fused-rmsnorm-quant-and-silu-mul-quant">Llama: Fused RMSNorm+Quant and SiLU+Mul+Quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#deepseek-v2-v3-mla-fused-input-norm-qk-norm-fusion">DeepSeek V2/V3: MLA + Fused Input Norm + QK Norm Fusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#qwen3-moe-qk-norm-rope-cache-quant-fusion">Qwen3-MoE: QK Norm + RoPE + Cache + Quant Fusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#mtp-deepseek-multi-token-prediction">MTP: DeepSeek Multi-Token Prediction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_ops_guide.html">ATOM Model Operations Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#aiter-integration-overview">1. AITER Integration Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#aiter-kernel-mapping-table">AITER Kernel Mapping Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#linear-operations">2. Linear Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#class-hierarchy">2.1 Class Hierarchy</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#quantization-dispatch">2.2 Quantization Dispatch</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#tensor-parallel-sharding">2.3 Tensor Parallel Sharding</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#weight-processing">2.4 Weight Processing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#attention-operations">3. Attention Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#base-attention-base-attention-py">3.1 Base: <code class="docutils literal notranslate"><span class="pre">Attention</span></code> (<code class="docutils literal notranslate"><span class="pre">base_attention.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#multi-head-attention-attention-mha-py">3.2 Multi-Head Attention (<code class="docutils literal notranslate"><span class="pre">attention_mha.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#multi-head-latent-attention-attention-mla-py">3.3 Multi-head Latent Attention (<code class="docutils literal notranslate"><span class="pre">attention_mla.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#backend-abstraction-attentions-backends-py">3.4 Backend Abstraction (<code class="docutils literal notranslate"><span class="pre">attentions/backends.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#kv-cache-operations">3.5 KV Cache Operations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#mixture-of-experts-moe">4. Mixture of Experts (MoE)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#fusedmoe-class-moe-py">4.1 <code class="docutils literal notranslate"><span class="pre">FusedMoE</span></code> Class (<code class="docutils literal notranslate"><span class="pre">moe.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#quantization-methods">4.2 Quantization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#topk-routing-topk-py">4.3 TopK Routing (<code class="docutils literal notranslate"><span class="pre">topK.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#fusedmoeparallelconfig">4.4 <code class="docutils literal notranslate"><span class="pre">FusedMoEParallelConfig</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#mori-integration-fused-moe-mori-prepare-finalize-py">4.5 MORI Integration (<code class="docutils literal notranslate"><span class="pre">fused_moe/mori_prepare_finalize.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#moe-quantization-config-fused-moe-config-py">4.6 MoE Quantization Config (<code class="docutils literal notranslate"><span class="pre">fused_moe/config.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#triton-moe-fallback-fused-moe-triton-py">4.7 Triton MoE Fallback (<code class="docutils literal notranslate"><span class="pre">fused_moe_triton.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#normalization">5. Normalization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#rmsnorm-layernorm-py">5.1 <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code> (<code class="docutils literal notranslate"><span class="pre">layernorm.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#layernorm-layernorm-py">5.2 <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> (<code class="docutils literal notranslate"><span class="pre">layernorm.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#activation-functions">6. Activation Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#siluandmul-activation-py">6.1 <code class="docutils literal notranslate"><span class="pre">SiluAndMul</span></code> (<code class="docutils literal notranslate"><span class="pre">activation.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#embedding-output-head">7. Embedding &amp; Output Head</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#vocabparallelembedding-embed-head-py">7.1 <code class="docutils literal notranslate"><span class="pre">VocabParallelEmbedding</span></code> (<code class="docutils literal notranslate"><span class="pre">embed_head.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#parallellmhead-embed-head-py">7.2 <code class="docutils literal notranslate"><span class="pre">ParallelLMHead</span></code> (<code class="docutils literal notranslate"><span class="pre">embed_head.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#rotary-position-embedding-rope">8. Rotary Position Embedding (RoPE)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#rotaryembedding-rotary-embedding-py">8.1 <code class="docutils literal notranslate"><span class="pre">RotaryEmbedding</span></code> (<code class="docutils literal notranslate"><span class="pre">rotary_embedding.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#get-rope-factory">8.2 <code class="docutils literal notranslate"><span class="pre">get_rope()</span></code> Factory</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#integration-in-attention">8.3 Integration in Attention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#sampling">9. Sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#sampler-sampler-py">9.1 <code class="docutils literal notranslate"><span class="pre">Sampler</span></code> (<code class="docutils literal notranslate"><span class="pre">sampler.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#rejectionsampler-rejection-sampler-py">9.2 <code class="docutils literal notranslate"><span class="pre">RejectionSampler</span></code> (<code class="docutils literal notranslate"><span class="pre">rejection_sampler.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#fused-kernel-chains">10. Fused Kernel Chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#source-files">Source Files</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-model-ops"><code class="docutils literal notranslate"><span class="pre">atom/model_ops/</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-model-ops-attentions"><code class="docutils literal notranslate"><span class="pre">atom/model_ops/attentions/</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-model-ops-fused-moe"><code class="docutils literal notranslate"><span class="pre">atom/model_ops/fused_moe/</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-utils"><code class="docutils literal notranslate"><span class="pre">atom/utils/</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="scheduling_kv_cache_guide.html">ATOM Scheduling &amp; KV Cache Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduling-algorithm">1. Scheduling Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduler-initialization">1.1 Scheduler Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#schedule-flow">1.2 Schedule Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#delay-factor">1.3 Delay Factor</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#preemption">1.4 Preemption</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduledbatch-structure">2. ScheduledBatch Structure</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#constructor-signature">2.1 Constructor Signature</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#fields">2.2 Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduledbatchoutput">2.3 ScheduledBatchOutput</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#block-manager">3. Block Manager</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#block-class">3.1 Block Class</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#blockmanager-initialization">3.2 BlockManager Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#allocation-allocate">3.3 Allocation (<code class="docutils literal notranslate"><span class="pre">allocate</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#deallocation-deallocate">3.4 Deallocation (<code class="docutils literal notranslate"><span class="pre">deallocate</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#can-allocate-and-can-append-checks">3.5 Can-Allocate and Can-Append Checks</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#may-append-decode-extension">3.6 May-Append (Decode Extension)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#prefix-caching">4. Prefix Caching</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#hash-function">4.1 Hash Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#hash-chaining">4.2 Hash Chaining</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#cache-lookup-during-allocation">4.3 Cache Lookup During Allocation</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#reference-counting">4.4 Reference Counting</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#enabling-prefix-caching">4.5 Enabling Prefix Caching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#postprocessing">5. Postprocessing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#signature">5.1 Signature</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#token-appending">5.2 Token Appending</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#stop-condition-checking">5.3 Stop Condition Checking</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#stream-output">5.4 Stream Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequence-cleanup">5.5 Sequence Cleanup</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#placeholder-insertion">5.6 Placeholder Insertion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#speculative-decoding-integration">6. Speculative Decoding Integration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduler-tracking">6.1 Scheduler Tracking</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#draft-tokens-in-scheduling">6.2 Draft Tokens in Scheduling</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#acceptance-statistics">6.3 Acceptance Statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#draft-token-storage-on-sequences">6.4 Draft Token Storage on Sequences</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequence-management">7. Sequence Management</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#constructor">7.1 Constructor</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#core-fields">7.2 Core Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#timing-fields">7.3 Timing Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#computed-properties">7.4 Computed Properties</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#num-tokens-setter">7.5 num_tokens Setter</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#lifecycle">7.6 Lifecycle</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequencestatus-enum">7.7 SequenceStatus Enum</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequencetype-enum">7.8 SequenceType Enum</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">ATOM Distributed Inference Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tensor-parallelism-tp">1. Tensor Parallelism (TP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#weight-sharding">Weight Sharding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#process-group-initialization">Process Group Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#allreduce">AllReduce</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuration">Configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-parallelism-dp">2. Data Parallelism (DP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#architecture">Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dp-process-group-initialization">DP Process Group Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#synchronized-busy-loop">Synchronized Busy Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dummy-batch-execution">Dummy Batch Execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#device-assignment">Device Assignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dpmetadata">DPMetadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="#coremanager-dp-orchestration">CoreManager (DP Orchestration)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#expert-parallelism-ep">3. Expert Parallelism (EP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#fusedmoeparallelconfig">FusedMoEParallelConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="#expert-distribution">Expert Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mori-communication">MORI Communication</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#environment-variables">4. Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multi-gpu-deployment-examples">5. Multi-GPU Deployment Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#deepseek-r1-on-8-gpus-tp8">DeepSeek-R1 on 8 GPUs (TP8)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qwen3-235b-a22b-on-8-gpus-tp8-ep">Qwen3-235B-A22B on 8 GPUs (TP8 + EP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kimi-k2-thinking-on-4-gpus-tp4">Kimi-K2-Thinking on 4 GPUs (TP4)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#combined-parallelism-strategies">6. Combined Parallelism Strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tp-only-dense-models">TP Only (Dense Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tp-ep-moe-models">TP + EP (MoE Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tp-dp-dense-throughput">TP + DP (Dense Throughput)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tp-dp-ep-moe-throughput">TP + DP + EP (MoE Throughput)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dp-attention-mode">DP Attention Mode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="compilation_cudagraph_guide.html">ATOM Compilation &amp; CUDA Graphs Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#compilation-levels">1. Compilation Levels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-0-no-compilation">Level 0 – NO_COMPILATION</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-1-dynamo-as-is">Level 1 – DYNAMO_AS_IS</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-2-dynamo-once">Level 2 – DYNAMO_ONCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-3-piecewise-production-default">Level 3 – PIECEWISE (Production Default)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#cuda-graph-modes">2. CUDA Graph Modes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#none-value-0">NONE (value: 0)</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#piecewise-value-1">PIECEWISE (value: 1)</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#full-value-2">FULL (value: 2)</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#full-decode-only-value-full-none">FULL_DECODE_ONLY (value: (FULL, NONE))</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#full-and-piecewise-value-full-piecewise">FULL_AND_PIECEWISE (value: (FULL, PIECEWISE))</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#helper-methods">Helper Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#cuda-graph-capture">3. CUDA Graph Capture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#capture-flow">Capture Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#graph-keying">Graph Keying</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#graph-pool-sharing">Graph Pool Sharing</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#default-capture-sizes">Default Capture Sizes</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#graph-replay-in-run-model">Graph Replay in run_model()</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#piecewise-compilation">4. Piecewise Compilation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#splitting-operations">Splitting Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#compilation-pipeline">Compilation Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#cache-management">Cache Management</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#forward-context-stateless-dispatch">5. Forward Context &amp; Stateless Dispatch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#forwardcontext-fields">ForwardContext Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#lifecycle">Lifecycle</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#context-dataclass">Context Dataclass</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#integration-with-cuda-graphs">Integration with CUDA Graphs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#compiler-backend">6. Compiler Backend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#compilermanager">CompilerManager</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#compilerinterface">CompilerInterface</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#inductoradaptor">InductorAdaptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#inductorstandaloneadaptor">InductorStandaloneAdaptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#vllmbackend">VllmBackend</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#support-torch-compile-decorator">&#64;support_torch_compile Decorator</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#custom-op-registration">Custom Op Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#configuration-options">7. Configuration Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#decision-tree">8. Decision Tree</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#common-configurations">Common Configurations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="serving_benchmarking_guide.html">ATOM Serving &amp; Benchmarking Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#openai-compatible-server">1. OpenAI-Compatible Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#endpoints">1.1 Endpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#request-models">1.2 Request Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#response-models">1.3 Response Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#server-startup">1.4 Server Startup</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#example-curl">1.5 Example: curl</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#programmatic-api-llmengine">2. Programmatic API (LLMEngine)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#initialization">2.1 Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#samplingparams">2.2 SamplingParams</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#core-methods">2.3 Core Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#synchronous-generation-example">2.4 Synchronous Generation Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#asynchronous-streaming-usage">2.5 Asynchronous / Streaming Usage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#simple-inference">3. Simple Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#usage">3.1 Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#what-it-does">3.2 What It Does</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#benchmarking">4. Benchmarking</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#metrics">4.1 Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#key-cli-arguments">4.2 Key CLI Arguments</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#backend-request-functions">4.3 Backend Request Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#full-benchmark-example">4.4 Full Benchmark Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#profiling">5. Profiling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#configuration">5.1 Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#online-profiling-http">5.2 Online Profiling (HTTP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#programmatic-profiling">5.3 Programmatic Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#offline-profiling-script">5.4 Offline Profiling Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#profiling-during-benchmarks">5.5 Profiling During Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#speculative-decoding-mtp">6. Speculative Decoding (MTP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#architecture">6.1 Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#id1">6.2 Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#mtp-statistics">6.3 MTP Statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#how-rejection-sampling-works">6.4 How Rejection Sampling Works</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#deployment-examples">7. Deployment Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#single-gpu">7.1 Single-GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#multi-gpu-with-tensor-parallelism">7.2 Multi-GPU with Tensor Parallelism</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#docker-deployment">7.3 Docker Deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#engine-cli-arguments-engineargs">7.4 Engine CLI Arguments (EngineArgs)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#accuracy-validation">8. Accuracy Validation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#setup">8.1 Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#run-evaluation">8.2 Run Evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#source-files">Source Files</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api/serving.html">Serving API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#llm-class">LLM Class</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/serving.html#methods">Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/serving.html#generate">generate()</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#samplingparams">SamplingParams</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#requestoutput">RequestOutput</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#example">Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/models.html">Supported Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#llama-models">Llama Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#gpt-models">GPT Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#mixtral">Mixtral</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#other-architectures">Other Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#performance-by-model-size">Performance by Model Size</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#quantization">Quantization</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #C00000" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ATOM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">ATOM Distributed Inference Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/distributed_guide.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="atom-distributed-inference-guide">
<h1>ATOM Distributed Inference Guide<a class="headerlink" href="#atom-distributed-inference-guide" title="Link to this heading"></a></h1>
<p>ATOM (AiTer Optimized Model) supports three parallelism strategies for distributed LLM inference on AMD ROCm/HIP GPUs: Tensor Parallelism (TP), Data Parallelism (DP), and Expert Parallelism (EP). These can be combined to scale across multiple GPUs for large model serving.</p>
<section id="quick-reference">
<h2>Quick Reference<a class="headerlink" href="#quick-reference" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parallelism</p></th>
<th class="head"><p>CLI Flag</p></th>
<th class="head"><p>Purpose</p></th>
<th class="head"><p>Communication</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Tensor Parallel (TP)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-tp</span> <span class="pre">N</span></code> / <code class="docutils literal notranslate"><span class="pre">--tensor-parallel-size</span> <span class="pre">N</span></code></p></td>
<td><p>Shard weights across GPUs</p></td>
<td><p>NCCL AllReduce</p></td>
</tr>
<tr class="row-odd"><td><p>Data Parallel (DP)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-dp</span> <span class="pre">N</span></code> / <code class="docutils literal notranslate"><span class="pre">--data-parallel-size</span> <span class="pre">N</span></code></p></td>
<td><p>Replicate model, split requests</p></td>
<td><p>Gloo AllReduce (CPU)</p></td>
</tr>
<tr class="row-even"><td><p>Expert Parallel (EP)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--enable-expert-parallel</span></code></p></td>
<td><p>Distribute MoE experts across GPUs</p></td>
<td><p>MORI All-to-All</p></td>
</tr>
<tr class="row-odd"><td><p>DP Attention</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--enable-dp-attention</span></code></p></td>
<td><p>Flatten DP into TP for MoE layers</p></td>
<td><p>NCCL AllGather/ReduceScatter</p></td>
</tr>
</tbody>
</table>
<p><strong>Common configurations:</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model Type</p></th>
<th class="head"><p>Configuration</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense (Llama, Qwen3)</p></td>
<td><p>TP only</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-tp</span> <span class="pre">8</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>MoE (Qwen3-235B)</p></td>
<td><p>TP + EP</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-tp</span> <span class="pre">8</span> <span class="pre">--enable-expert-parallel</span></code></p></td>
</tr>
<tr class="row-even"><td><p>MoE throughput scaling</p></td>
<td><p>TP + DP + EP</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-tp</span> <span class="pre">4</span> <span class="pre">-dp</span> <span class="pre">2</span> <span class="pre">--enable-expert-parallel</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense throughput scaling</p></td>
<td><p>TP + DP</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-tp</span> <span class="pre">4</span> <span class="pre">-dp</span> <span class="pre">2</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="tensor-parallelism-tp">
<h2>1. Tensor Parallelism (TP)<a class="headerlink" href="#tensor-parallelism-tp" title="Link to this heading"></a></h2>
<p>Tensor Parallelism shards model weights across GPUs so each GPU holds a slice of every layer. ATOM uses AITER’s <code class="docutils literal notranslate"><span class="pre">init_dist_env()</span></code> to initialize NCCL process groups.</p>
<section id="weight-sharding">
<h3>Weight Sharding<a class="headerlink" href="#weight-sharding" title="Link to this heading"></a></h3>
<p>ATOM provides parallel linear layer classes in <code class="docutils literal notranslate"><span class="pre">atom/model_ops/linear.py</span></code>:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">ColumnParallelLinear</span></code></strong> – splits the output dimension (dim 0) across TP ranks. Each GPU computes a shard of the output independently.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">RowParallelLinear</span></code></strong> – splits the input dimension (dim 1) across TP ranks. After the local matmul, an AllReduce across the TP group aggregates partial results.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">QKVParallelLinear</span></code></strong> – extends <code class="docutils literal notranslate"><span class="pre">ColumnParallelLinear</span></code> for attention Q/K/V projections. Partitions heads across TP ranks, replicating KV heads when <code class="docutils literal notranslate"><span class="pre">num_kv_heads</span> <span class="pre">&lt;</span> <span class="pre">tp_size</span></code>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">MergedColumnParallelLinear</span></code></strong> – merges multiple column-parallel outputs (e.g., gate and up projections) into a single weight tensor, sharded along dim 0.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">ReplicatedLinear</span></code></strong> – no sharding; weight is replicated on every rank.</p></li>
</ul>
</section>
<section id="process-group-initialization">
<h3>Process Group Initialization<a class="headerlink" href="#process-group-initialization" title="Link to this heading"></a></h3>
<p>In <code class="docutils literal notranslate"><span class="pre">ModelRunner.__init__()</span></code>, the distributed environment is set up via AITER:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">aiter</span><span class="w"> </span><span class="kn">import</span> <span class="n">init_dist_env</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aiter.dist.parallel_state</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_tp_group</span><span class="p">,</span> <span class="n">get_dp_group</span><span class="p">,</span> <span class="n">get_pp_group</span>

<span class="n">init_dist_env</span><span class="p">(</span>
    <span class="n">config</span><span class="o">.</span><span class="n">tensor_parallel_size</span><span class="p">,</span>
    <span class="n">rankID</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
    <span class="n">distributed_init_method</span><span class="o">=</span><span class="n">distributed_init_method</span><span class="p">,</span>
    <span class="n">data_parallel_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel_size</span><span class="p">,</span>
    <span class="n">data_parallel_rank</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel_rank</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>After initialization, <code class="docutils literal notranslate"><span class="pre">get_tp_group()</span></code>, <code class="docutils literal notranslate"><span class="pre">get_dp_group()</span></code>, and <code class="docutils literal notranslate"><span class="pre">get_pp_group()</span></code> provide the respective process groups for collective operations.</p>
</section>
<section id="allreduce">
<h3>AllReduce<a class="headerlink" href="#allreduce" title="Link to this heading"></a></h3>
<p>The AllReduce happens inside <code class="docutils literal notranslate"><span class="pre">LinearBase.forward()</span></code> when <code class="docutils literal notranslate"><span class="pre">tp_dim</span> <span class="pre">==</span> <span class="pre">1</span></code> (row-parallel):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tp_dim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tp_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_results</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">get_tp_group</span><span class="p">()</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ca_fp8_quant</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="configuration">
<h3>Configuration<a class="headerlink" href="#configuration" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Config.tensor_parallel_size</span></code> (int, default <code class="docutils literal notranslate"><span class="pre">1</span></code>): Number of TP ranks. Must satisfy <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">&lt;=</span> <span class="pre">tensor_parallel_size</span> <span class="pre">&lt;=</span> <span class="pre">8</span></code>.</p></li>
<li><p>CLI: <code class="docutils literal notranslate"><span class="pre">--tensor-parallel-size</span> <span class="pre">N</span></code> or <code class="docutils literal notranslate"><span class="pre">-tp</span> <span class="pre">N</span></code></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="data-parallelism-dp">
<h2>2. Data Parallelism (DP)<a class="headerlink" href="#data-parallelism-dp" title="Link to this heading"></a></h2>
<p>Data Parallelism runs multiple independent engine replicas, each handling a subset of incoming requests. DP is coordinated at the scheduling level rather than the model level – each DP rank has its own <code class="docutils literal notranslate"><span class="pre">EngineCore</span></code>, scheduler, and model runner.</p>
<section id="architecture">
<h3>Architecture<a class="headerlink" href="#architecture" title="Link to this heading"></a></h3>
<p>When <code class="docutils literal notranslate"><span class="pre">data_parallel_size</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">EngineCore.run_engine()</span></code> instantiates a <code class="docutils literal notranslate"><span class="pre">DPEngineCoreProc</span></code> instead of a plain <code class="docutils literal notranslate"><span class="pre">EngineCore</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># atom/model_engine/engine_core.py</span>
<span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">run_engine</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">input_address</span><span class="p">,</span> <span class="n">output_address</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">engine</span> <span class="o">=</span> <span class="n">DPEngineCoreProc</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">input_address</span><span class="p">,</span> <span class="n">output_address</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">engine</span> <span class="o">=</span> <span class="n">EngineCore</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">input_address</span><span class="p">,</span> <span class="n">output_address</span><span class="p">)</span>
    <span class="n">engine</span><span class="o">.</span><span class="n">busy_loop</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="dp-process-group-initialization">
<h3>DP Process Group Initialization<a class="headerlink" href="#dp-process-group-initialization" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">DPEngineCoreProc._init_data_parallel()</span></code> creates a Gloo-based process group for CPU-side coordination:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_init_data_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">dp_rank</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel_rank</span>
    <span class="n">dp_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel_size</span>
    <span class="n">local_dp_rank</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel_rank_local</span>

    <span class="k">assert</span> <span class="n">dp_size</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">local_dp_rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dp_rank</span> <span class="o">=</span> <span class="n">dp_rank</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dp_group</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">stateless_init_dp_group</span><span class="p">()</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">stateless_init_dp_group()</span></code> method (in <code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code>) calls <code class="docutils literal notranslate"><span class="pre">stateless_init_torch_distributed_process_group()</span></code> with the <code class="docutils literal notranslate"><span class="pre">gloo</span></code> backend, creating an isolated process group that does not interfere with the NCCL TP group.</p>
</section>
<section id="synchronized-busy-loop">
<h3>Synchronized Busy Loop<a class="headerlink" href="#synchronized-busy-loop" title="Link to this heading"></a></h3>
<p>The DP busy loop overrides the base <code class="docutils literal notranslate"><span class="pre">EngineCore.busy_loop()</span></code> to synchronize state across DP ranks before each step. The <code class="docutils literal notranslate"><span class="pre">_sync_dp_state()</span></code> method packs four signals into an int64 tensor and performs a single <code class="docutils literal notranslate"><span class="pre">AllReduce(MAX)</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># State synced: [is_prefill, num_tokens, has_unfinished, shutdown]</span>
<span class="n">state_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="mi">1</span> <span class="k">if</span> <span class="n">local_is_prefill</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">local_num_tokens</span><span class="p">,</span>
        <span class="mi">1</span> <span class="k">if</span> <span class="n">local_has_unfinished</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
        <span class="mi">1</span> <span class="k">if</span> <span class="n">local_shutdown</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
    <span class="n">state_tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dp_group</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This ensures:</p>
<ul class="simple">
<li><p><strong>All ranks agree on the batch type</strong> (prefill vs. decode). Since MORI requires all DP ranks to execute the same phase, a rank that has no prefill work must run a dummy prefill when any other rank does prefill.</p></li>
<li><p><strong>Graceful shutdown</strong>: all ranks must agree before exiting.</p></li>
<li><p><strong>Token count alignment</strong>: the maximum token count across ranks is used for padding.</p></li>
</ul>
</section>
<section id="dummy-batch-execution">
<h3>Dummy Batch Execution<a class="headerlink" href="#dummy-batch-execution" title="Link to this heading"></a></h3>
<p>When a DP rank has no real work but other ranks do, it executes dummy batches to participate in collective operations:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">_execute_dummy_batch()</span></code></strong> – runs a 1-token decode dummy through the model, triggering AllReduce and MORI collectives so other ranks are not blocked.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">_execute_dummy_prefill(num_tokens)</span></code></strong> – runs a dummy prefill with the same token count as the max across DP ranks, so that MORI dispatch/combine stays synchronized.</p></li>
</ul>
</section>
<section id="device-assignment">
<h3>Device Assignment<a class="headerlink" href="#device-assignment" title="Link to this heading"></a></h3>
<p>When DP is enabled on a single node, each DP rank uses a different set of GPUs. The device mapping in <code class="docutils literal notranslate"><span class="pre">ModelRunner.__init__()</span></code> is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">local_device_rank</span> <span class="o">=</span> <span class="n">dp_rank_local</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">tensor_parallel_size</span> <span class="o">+</span> <span class="n">rank</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">local_device_rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>For example, with DP=2 and TP=4:</p>
<ul class="simple">
<li><p>DP rank 0: GPUs 0, 1, 2, 3</p></li>
<li><p>DP rank 1: GPUs 4, 5, 6, 7</p></li>
</ul>
</section>
<section id="dpmetadata">
<h3>DPMetadata<a class="headerlink" href="#dpmetadata" title="Link to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">DPMetadata</span></code> dataclass (in <code class="docutils literal notranslate"><span class="pre">atom/utils/forward_context.py</span></code>) tracks token distribution across DP ranks for padding and collective operations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DPMetadata</span><span class="p">:</span>
    <span class="n">max_tokens_across_dp_cpu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>   <span class="c1"># Max tokens on any DP rank</span>
    <span class="n">cu_tokens_across_dp_cpu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>    <span class="c1"># Cumulative token counts</span>
    <span class="n">max_tokens_across_dp</span><span class="p">:</span> <span class="nb">int</span>                <span class="c1"># Pre-computed int for CUDA graph</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">DPMetadata.num_tokens_across_dp()</span></code> gathers token counts via an AllReduce on the DP CPU group:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_tokens_across_dp</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dp_size</span>
<span class="n">num_tokens_across_dp</span><span class="p">[</span><span class="n">dp_rank</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_tokens</span>
<span class="n">num_tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">num_tokens_across_dp</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">num_tokens_tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">get_dp_group</span><span class="p">()</span><span class="o">.</span><span class="n">cpu_group</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="coremanager-dp-orchestration">
<h3>CoreManager (DP Orchestration)<a class="headerlink" href="#coremanager-dp-orchestration" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">CoreManager</span></code> (in <code class="docutils literal notranslate"><span class="pre">atom/model_engine/engine_core_mgr.py</span></code>) manages multiple DP engine processes:</p>
<ol class="arabic simple">
<li><p>For each DP rank, it creates a <code class="docutils literal notranslate"><span class="pre">Config</span></code> copy with the appropriate <code class="docutils literal notranslate"><span class="pre">data_parallel_rank</span></code> and <code class="docutils literal notranslate"><span class="pre">data_parallel_rank_local</span></code>.</p></li>
<li><p>Launches each <code class="docutils literal notranslate"><span class="pre">EngineCore</span></code> in a separate <code class="docutils literal notranslate"><span class="pre">multiprocessing.Process</span></code>.</p></li>
<li><p>Uses ZMQ (ROUTER/DEALER) sockets for input distribution and ZMQ (PUSH/PULL) for output collection.</p></li>
<li><p>Distributes incoming requests across DP ranks via round-robin load balancing.</p></li>
<li><p>Waits for READY signals from all ranks before accepting requests.</p></li>
</ol>
<p>When <code class="docutils literal notranslate"><span class="pre">enable_dp_attention</span></code> is set, <code class="docutils literal notranslate"><span class="pre">CoreManager</span></code> flattens TP into DP:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">enable_dp_attention</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">local_engine_count</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">tensor_parallel_size</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel_size</span>
    <span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_engine_count</span>
    <span class="n">config</span><span class="o">.</span><span class="n">tensor_parallel_size</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</section>
<section id="id1">
<h3>Configuration<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ParallelConfig.data_parallel_size</span></code> (int, default <code class="docutils literal notranslate"><span class="pre">1</span></code>): Number of DP replicas.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ParallelConfig.data_parallel_rank</span></code> (int, default <code class="docutils literal notranslate"><span class="pre">0</span></code>): This rank’s DP index.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ParallelConfig.data_parallel_rank_local</span></code> (int, default <code class="docutils literal notranslate"><span class="pre">None</span></code>): Local DP rank on this node.</p></li>
<li><p>CLI: <code class="docutils literal notranslate"><span class="pre">--data-parallel-size</span> <span class="pre">N</span></code> or <code class="docutils literal notranslate"><span class="pre">-dp</span> <span class="pre">N</span></code></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="expert-parallelism-ep">
<h2>3. Expert Parallelism (EP)<a class="headerlink" href="#expert-parallelism-ep" title="Link to this heading"></a></h2>
<p>Expert Parallelism distributes MoE experts across GPUs so that each GPU owns a subset of experts. Tokens are routed to the correct GPU via all-to-all communication.</p>
<section id="fusedmoeparallelconfig">
<h3>FusedMoEParallelConfig<a class="headerlink" href="#fusedmoeparallelconfig" title="Link to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">FusedMoEParallelConfig</span></code> dataclass (in <code class="docutils literal notranslate"><span class="pre">atom/model_ops/moe.py</span></code>) determines how MoE layers are parallelized:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FusedMoEParallelConfig</span><span class="p">:</span>
    <span class="n">tp_size</span><span class="p">:</span> <span class="nb">int</span>        <span class="c1"># Tensor parallel size (1 when EP is active)</span>
    <span class="n">dp_size</span><span class="p">:</span> <span class="nb">int</span>        <span class="c1"># Data parallel size</span>
    <span class="n">ep_size</span><span class="p">:</span> <span class="nb">int</span>        <span class="c1"># Expert parallel size</span>
    <span class="n">tp_rank</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dp_rank</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">ep_rank</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">use_ep</span><span class="p">:</span> <span class="nb">bool</span>        <span class="c1"># Whether EP is enabled</span>
    <span class="n">local_ep_size</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># Number of EP ranks on this node</span>
</pre></div>
</div>
<p>Key properties:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">use_all2all_kernels</span></code></strong>: returns <code class="docutils literal notranslate"><span class="pre">True</span></code> when <code class="docutils literal notranslate"><span class="pre">dp_size</span> <span class="pre">&gt;</span> <span class="pre">1</span> <span class="pre">and</span> <span class="pre">use_ep</span> <span class="pre">and</span> <span class="pre">mori</span> <span class="pre">is</span> <span class="pre">available</span></code>. This activates the MORI all-to-all dispatch/combine kernels.</p></li>
<li><p>When EP is enabled, <code class="docutils literal notranslate"><span class="pre">tp_size</span></code> is set to 1 and <code class="docutils literal notranslate"><span class="pre">ep_size</span> <span class="pre">=</span> <span class="pre">dp_size</span> <span class="pre">*</span> <span class="pre">tp_size</span></code> (the original TP size). Each device fully owns its assigned experts.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">FusedMoEParallelConfig.make()</span></code> static method constructs the config:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">use_ep</span> <span class="o">=</span> <span class="n">dp_size_</span> <span class="o">*</span> <span class="n">tp_size_</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">enable_expert_parallel</span>

<span class="k">if</span> <span class="n">enable_dp_attention</span><span class="p">:</span>
    <span class="c1"># Flatten DP into TP: effective tp_size = dp_size * tp_size</span>
    <span class="n">tp_size</span><span class="p">,</span> <span class="n">tp_rank</span> <span class="o">=</span> <span class="n">flatten_tp_across_dp</span><span class="p">(</span><span class="n">dp_rank</span><span class="p">)</span>

<span class="k">if</span> <span class="n">use_ep</span><span class="p">:</span>
    <span class="n">ep_size</span> <span class="o">=</span> <span class="n">tp_size</span>
    <span class="n">ep_rank</span> <span class="o">=</span> <span class="n">tp_rank</span>
    <span class="c1"># Each device owns experts fully -- no intra-expert tensor parallelism</span>
    <span class="k">return</span> <span class="n">FusedMoEParallelConfig</span><span class="p">(</span><span class="n">tp_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tp_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ep_size</span><span class="o">=</span><span class="n">ep_size</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="expert-distribution">
<h3>Expert Distribution<a class="headerlink" href="#expert-distribution" title="Link to this heading"></a></h3>
<p>In <code class="docutils literal notranslate"><span class="pre">FusedMoE.__init__()</span></code>, when EP is active, the global experts are partitioned:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_ep</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">local_num_experts</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_map</span> <span class="o">=</span> <span class="n">determine_expert_map</span><span class="p">(</span>
        <span class="n">ep_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_size</span><span class="p">,</span>
        <span class="n">ep_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rank</span><span class="p">,</span>
        <span class="n">global_num_experts</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_num_experts</span><span class="p">,</span>
    <span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">local_num_experts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_num_experts</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expert_map</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>Each GPU only loads weights for its assigned experts, reducing per-GPU memory usage proportionally.</p>
</section>
<section id="mori-communication">
<h3>MORI Communication<a class="headerlink" href="#mori-communication" title="Link to this heading"></a></h3>
<p>When <code class="docutils literal notranslate"><span class="pre">use_all2all_kernels</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the <code class="docutils literal notranslate"><span class="pre">MoriPrepareAndFinalize</span></code> class (in <code class="docutils literal notranslate"><span class="pre">atom/model_ops/fused_moe/mori_prepare_finalize.py</span></code>) handles token routing:</p>
<p><strong>Dispatch phase</strong> (<code class="docutils literal notranslate"><span class="pre">prepare()</span></code>):</p>
<ol class="arabic simple">
<li><p>Receives input activations, top-k weights, and top-k expert IDs.</p></li>
<li><p>Calls <code class="docutils literal notranslate"><span class="pre">self.mori_op.dispatch()</span></code> to send each token to the GPU that owns its selected expert.</p></li>
<li><p>Returns dispatched activations, scales, expert IDs, weights, and per-expert token counts.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">dispatch_a1</span><span class="p">,</span> <span class="n">dispatch_weights</span><span class="p">,</span> <span class="n">dispatch_scale</span><span class="p">,</span> <span class="n">dispatch_ids</span><span class="p">,</span> <span class="n">dispatch_recv_token_num</span>
<span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mori_op</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">topk_weights</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">topk_ids</span><span class="p">,</span> <span class="n">block_num</span><span class="p">,</span> <span class="n">warp_per_block</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Combine phase</strong> (<code class="docutils literal notranslate"><span class="pre">finalize()</span></code>):</p>
<ol class="arabic simple">
<li><p>After expert computation, calls <code class="docutils literal notranslate"><span class="pre">self.mori_op.combine()</span></code> to route results back to the originating GPU.</p></li>
<li><p>Copies the combined result into the output tensor.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mori_op</span><span class="o">.</span><span class="n">combine</span><span class="p">(</span><span class="n">fused_expert_output</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">topk_ids</span><span class="p">,</span> <span class="n">block_num</span><span class="p">,</span> <span class="n">warp_per_block</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">output</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">result</span><span class="p">[:</span><span class="n">num_token</span><span class="p">])</span>
</pre></div>
</div>
<p>The block configuration adapts to the batch type: prefill uses <code class="docutils literal notranslate"><span class="pre">block_num=128,</span> <span class="pre">warp_per_block=16</span></code>, while decode uses <code class="docutils literal notranslate"><span class="pre">block_num=64,</span> <span class="pre">warp_per_block=4</span></code>.</p>
</section>
<section id="id2">
<h3>Configuration<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Config.enable_expert_parallel</span></code> (bool, default <code class="docutils literal notranslate"><span class="pre">False</span></code>): Activates EP for MoE layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Config.enable_dp_attention</span></code> (bool, default <code class="docutils literal notranslate"><span class="pre">False</span></code>): Flattens DP ranks into the TP/EP dimension for MoE, while using per-rank attention for non-MoE layers.</p></li>
<li><p>CLI: <code class="docutils literal notranslate"><span class="pre">--enable-expert-parallel</span></code>, <code class="docutils literal notranslate"><span class="pre">--enable-dp-attention</span></code></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="environment-variables">
<h2>4. Environment Variables<a class="headerlink" href="#environment-variables" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Variable</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_RANK</span></code></p></td>
<td><p>int</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p>Data parallel rank index</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_RANK_LOCAL</span></code></p></td>
<td><p>int</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p>Local data parallel rank on this node</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_SIZE</span></code></p></td>
<td><p>int</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Total number of data parallel replicas</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_MASTER_IP</span></code></p></td>
<td><p>str</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code></p></td>
<td><p>IP address for DP Gloo rendezvous</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_MASTER_PORT</span></code></p></td>
<td><p>int</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">29500</span></code></p></td>
<td><p>Port for DP Gloo rendezvous</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_ENFORCE_EAGER</span></code></p></td>
<td><p>bool</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Disable CUDA graphs (set automatically)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_QK_NORM_ROPE_CACHE_QUANT_FUSION</span></code></p></td>
<td><p>bool</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Fuse QK-norm + RoPE + cache quant (for Qwen3-MoE)</p></td>
</tr>
</tbody>
</table>
<p>Environment variables in <code class="docutils literal notranslate"><span class="pre">atom/utils/envs.py</span></code> are evaluated lazily via <code class="docutils literal notranslate"><span class="pre">__getattr__</span></code>. If <code class="docutils literal notranslate"><span class="pre">ATOM_DP_SIZE</span></code>, <code class="docutils literal notranslate"><span class="pre">ATOM_DP_RANK</span></code>, or <code class="docutils literal notranslate"><span class="pre">ATOM_DP_RANK_LOCAL</span></code> are set in the environment, they override programmatic <code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code> defaults in <code class="docutils literal notranslate"><span class="pre">ParallelConfig.__post_init__()</span></code>.</p>
<p><strong>AITER environment variable (not in envs.py):</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Variable</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">AITER_QUICK_REDUCE_QUANTIZATION</span></code></p></td>
<td><p>str</p></td>
<td><p>–</p></td>
<td><p>Set to <code class="docutils literal notranslate"><span class="pre">INT4</span></code> to enable quantized AllReduce for prefill (read by AITER’s AllReduce kernel)</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="multi-gpu-deployment-examples">
<h2>5. Multi-GPU Deployment Examples<a class="headerlink" href="#multi-gpu-deployment-examples" title="Link to this heading"></a></h2>
<section id="deepseek-r1-on-8-gpus-tp8">
<h3>DeepSeek-R1 on 8 GPUs (TP8)<a class="headerlink" href="#deepseek-r1-on-8-gpus-tp8" title="Link to this heading"></a></h3>
<p>From the project README – a dense MLA model deployed with pure tensor parallelism:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>atom.entrypoints.openai_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--kv_cache_dtype<span class="w"> </span>fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-tp<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>deepseek-ai/DeepSeek-R1
</pre></div>
</div>
</section>
<section id="qwen3-235b-a22b-on-8-gpus-tp8-ep">
<h3>Qwen3-235B-A22B on 8 GPUs (TP8 + EP)<a class="headerlink" href="#qwen3-235b-a22b-on-8-gpus-tp8-ep" title="Link to this heading"></a></h3>
<p>From <code class="docutils literal notranslate"><span class="pre">recipes/Qwen3-235b.md</span></code> – a MoE model with 128 experts, deployed with tensor parallelism and expert parallelism:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">AITER_QUICK_REDUCE_QUANTIZATION</span><span class="o">=</span>INT4
<span class="nb">export</span><span class="w"> </span><span class="nv">ATOM_ENABLE_QK_NORM_ROPE_CACHE_QUANT_FUSION</span><span class="o">=</span><span class="m">1</span>

python<span class="w"> </span>-m<span class="w"> </span>atom.entrypoints.openai_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen3-235B-A22B-Instruct-2507-FP8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-tp<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--kv_cache_dtype<span class="w"> </span>fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--enable-expert-parallel<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-model-len<span class="w"> </span><span class="m">16384</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-num-batched-tokens<span class="w"> </span><span class="m">20000</span>
</pre></div>
</div>
<p>Tips from the recipe:</p>
<ul class="simple">
<li><p>Use FP8 KV cache (<code class="docutils literal notranslate"><span class="pre">--kv_cache_dtype</span> <span class="pre">fp8</span></code>) for memory efficiency.</p></li>
<li><p>Quick AllReduce with INT4 quantization reduces prefill TTFT.</p></li>
<li><p>QK-norm + RoPE + cache quant fusion improves Qwen3-MoE kernel performance.</p></li>
</ul>
</section>
<section id="kimi-k2-thinking-on-4-gpus-tp4">
<h3>Kimi-K2-Thinking on 4 GPUs (TP4)<a class="headerlink" href="#kimi-k2-thinking-on-4-gpus-tp4" title="Link to this heading"></a></h3>
<p>From <code class="docutils literal notranslate"><span class="pre">recipes/Kimi-K2-Thinking.md</span></code> – an MXFP4 MoE model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">HIP_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3

python<span class="w"> </span>-m<span class="w"> </span>atom.entrypoints.openai_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>amd/Kimi-K2-Thinking-MXFP4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-tp<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--kv_cache_dtype<span class="w"> </span>fp8
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="combined-parallelism-strategies">
<h2>6. Combined Parallelism Strategies<a class="headerlink" href="#combined-parallelism-strategies" title="Link to this heading"></a></h2>
<section id="tp-only-dense-models">
<h3>TP Only (Dense Models)<a class="headerlink" href="#tp-only-dense-models" title="Link to this heading"></a></h3>
<p>For dense models like Llama and Qwen3 (non-MoE), use pure tensor parallelism:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>atom.entrypoints.openai_server<span class="w"> </span>--model<span class="w"> </span>meta-llama/Meta-Llama-3-8B<span class="w"> </span>-tp<span class="w"> </span><span class="m">8</span>
</pre></div>
</div>
<p>All weights are sharded across GPUs. AllReduce collectives synchronize after each <code class="docutils literal notranslate"><span class="pre">RowParallelLinear</span></code>.</p>
</section>
<section id="tp-ep-moe-models">
<h3>TP + EP (MoE Models)<a class="headerlink" href="#tp-ep-moe-models" title="Link to this heading"></a></h3>
<p>For MoE models, enable expert parallelism so each GPU holds a subset of experts:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>atom.entrypoints.openai_server<span class="w"> </span>--model<span class="w"> </span>Qwen/Qwen3-235B-A22B-Instruct-2507-FP8<span class="w"> </span>-tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--enable-expert-parallel
</pre></div>
</div>
<p>Dense layers (attention, norms) remain tensor-parallel. MoE layers distribute experts across the <code class="docutils literal notranslate"><span class="pre">ep_size</span> <span class="pre">=</span> <span class="pre">tp_size</span></code> GPUs. MORI all-to-all routes tokens to the correct expert owner.</p>
</section>
<section id="tp-dp-dense-throughput">
<h3>TP + DP (Dense Throughput)<a class="headerlink" href="#tp-dp-dense-throughput" title="Link to this heading"></a></h3>
<p>For throughput scaling with dense models, run multiple DP replicas:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># On a node with 8 GPUs: 2 replicas, each using 4 GPUs</span>
python<span class="w"> </span>-m<span class="w"> </span>atom.entrypoints.openai_server<span class="w"> </span>--model<span class="w"> </span>meta-llama/Meta-Llama-3-8B<span class="w"> </span>-tp<span class="w"> </span><span class="m">4</span><span class="w"> </span>-dp<span class="w"> </span><span class="m">2</span>
</pre></div>
</div>
<p>Each DP replica independently processes a subset of requests. The <code class="docutils literal notranslate"><span class="pre">CoreManager</span></code> distributes requests via round-robin. Device mapping:</p>
<ul class="simple">
<li><p>DP rank 0, TP ranks 0-3 –&gt; GPUs 0-3</p></li>
<li><p>DP rank 1, TP ranks 0-3 –&gt; GPUs 4-7</p></li>
</ul>
<p>Formula: <code class="docutils literal notranslate"><span class="pre">local_device_rank</span> <span class="pre">=</span> <span class="pre">dp_rank_local</span> <span class="pre">*</span> <span class="pre">tp_size</span> <span class="pre">+</span> <span class="pre">tp_rank</span></code></p>
</section>
<section id="tp-dp-ep-moe-throughput">
<h3>TP + DP + EP (MoE Throughput)<a class="headerlink" href="#tp-dp-ep-moe-throughput" title="Link to this heading"></a></h3>
<p>For MoE models with DP + EP, the expert parallel dimension spans all <code class="docutils literal notranslate"><span class="pre">tp_size</span> <span class="pre">*</span> <span class="pre">dp_size</span></code> devices:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>atom.entrypoints.openai_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen3-235B-A22B-Instruct-2507-FP8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-tp<span class="w"> </span><span class="m">4</span><span class="w"> </span>-dp<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--enable-expert-parallel
</pre></div>
</div>
<p>In this configuration:</p>
<ul class="simple">
<li><p>Dense layers: each DP replica has TP=4 for sharding.</p></li>
<li><p>MoE layers: EP size = <code class="docutils literal notranslate"><span class="pre">dp_size</span> <span class="pre">*</span> <span class="pre">tp_size</span> <span class="pre">=</span> <span class="pre">8</span></code>, spreading experts across all 8 GPUs.</p></li>
<li><p>MORI all-to-all crosses DP boundaries to route tokens to the correct expert owner.</p></li>
</ul>
</section>
<section id="dp-attention-mode">
<h3>DP Attention Mode<a class="headerlink" href="#dp-attention-mode" title="Link to this heading"></a></h3>
<p>When <code class="docutils literal notranslate"><span class="pre">--enable-dp-attention</span></code> is set, <code class="docutils literal notranslate"><span class="pre">CoreManager</span></code> flattens the TP dimension into DP:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">local_engine_count</span> <span class="o">=</span> <span class="n">tensor_parallel_size</span> <span class="o">*</span> <span class="n">data_parallel_size</span>
<span class="n">data_parallel_size</span> <span class="o">=</span> <span class="n">local_engine_count</span>
<span class="n">tensor_parallel_size</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>This means each GPU runs an independent attention computation (no TP AllReduce for attention), while MoE layers still use the full EP group across all GPUs. This can reduce communication overhead for attention-heavy workloads.</p>
</section>
</section>
<hr class="docutils" />
<section id="source-files">
<h2>Source Files<a class="headerlink" href="#source-files" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>File</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/config.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">Config.tensor_parallel_size</span></code>, <code class="docutils literal notranslate"><span class="pre">enable_expert_parallel</span></code>, <code class="docutils literal notranslate"><span class="pre">enable_dp_attention</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/utils/envs.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_*</span></code> environment variables (lazy evaluation)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_engine/engine_core.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">EngineCore</span></code>, <code class="docutils literal notranslate"><span class="pre">DPEngineCoreProc</span></code> (DP busy loop, sync, dummy batches)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_engine/engine_core_mgr.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CoreManager</span></code> (multi-process DP orchestration, ZMQ IPC)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_engine/model_runner.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ModelRunner</span></code> (<code class="docutils literal notranslate"><span class="pre">init_dist_env</span></code>, device assignment, <code class="docutils literal notranslate"><span class="pre">DPMetadata</span></code> usage)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_engine/arg_utils.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">EngineArgs</span></code> CLI argument definitions</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/utils/distributed/utils.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">stateless_init_torch_distributed_process_group()</span></code> (Gloo PG creation)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/utils/forward_context.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">DPMetadata</span></code>, <code class="docutils literal notranslate"><span class="pre">ForwardContext</span></code> (per-step DP token metadata)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_ops/linear.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ColumnParallelLinear</span></code>, <code class="docutils literal notranslate"><span class="pre">RowParallelLinear</span></code>, <code class="docutils literal notranslate"><span class="pre">QKVParallelLinear</span></code>, <code class="docutils literal notranslate"><span class="pre">MergedColumnParallelLinear</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_ops/moe.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">FusedMoE</span></code>, <code class="docutils literal notranslate"><span class="pre">FusedMoEParallelConfig</span></code> (EP configuration and expert distribution)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_ops/fused_moe/mori_prepare_finalize.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MoriPrepareAndFinalize</span></code> (MORI dispatch/combine for EP)</p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="scheduling_kv_cache_guide.html" class="btn btn-neutral float-left" title="ATOM Scheduling &amp; KV Cache Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="compilation_cudagraph_guide.html" class="btn btn-neutral float-right" title="ATOM Compilation &amp; CUDA Graphs Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, AMD.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>