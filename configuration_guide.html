

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ATOM Configuration Guide &mdash; ATOM 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=9edc463e" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=01f34227"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="ATOM Model Support Guide" href="model_support_guide.html" />
    <link rel="prev" title="ATOM Architecture Guide" href="architecture_guide.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #C00000" >

          
          
          <a href="index.html" class="icon icon-home">
            ATOM
              <img src="_static/atom_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#installation-methods">Installation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="installation.html#from-source">From Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="installation.html#docker-installation">Docker Installation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#environment-variables">Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#verification">Verification</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#serving-a-model">Serving a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#batch-inference">Batch Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#distributed-serving">Distributed Serving</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#api-server">API Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#performance-tips">Performance Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#next-steps">Next Steps</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture_guide.html">ATOM Architecture Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#system-overview">1. System Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#component-architecture">2. Component Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#request-lifecycle">3. Request Lifecycle</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#forward-context-pattern">4. Forward Context Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#multi-process-architecture">5. Multi-Process Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#sequence-lifecycle">6. Sequence Lifecycle</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">ATOM Configuration Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#master-configuration-config">1. Master Configuration (<code class="docutils literal notranslate"><span class="pre">Config</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compilation-configuration-compilationconfig">2. Compilation Configuration (<code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#compilation-levels-compilationlevel">2.1 Compilation Levels (<code class="docutils literal notranslate"><span class="pre">CompilationLevel</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compilationconfig-fields">2.2 <code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code> Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cuda-graph-mode-cudagraphmode">2.3 CUDA Graph Mode (<code class="docutils literal notranslate"><span class="pre">CUDAGraphMode</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#quantization-configuration-quantizationconfig">3. Quantization Configuration (<code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quantizationconfig-fields">3.1 <code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code> Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quanttype-values-from-aiter">3.2 <code class="docutils literal notranslate"><span class="pre">QuantType</span></code> Values (from AITER)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#supported-quantization-dtypes">3.3 Supported Quantization Dtypes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#auto-detection-from-huggingface-get-quant-config">3.4 Auto-Detection from HuggingFace (<code class="docutils literal notranslate"><span class="pre">get_quant_config</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#parallel-configuration-parallelconfig">4. Parallel Configuration (<code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#speculative-decoding-configuration-speculativeconfig">5. Speculative Decoding Configuration (<code class="docutils literal notranslate"><span class="pre">SpeculativeConfig</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sampling-parameters-samplingparams">6. Sampling Parameters (<code class="docutils literal notranslate"><span class="pre">SamplingParams</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cli-arguments-engineargs">7. CLI Arguments (<code class="docutils literal notranslate"><span class="pre">EngineArgs</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#environment-variables">8. Environment Variables</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#variables-registered-in-atom-utils-envs-py">8.1 Variables Registered in <code class="docutils literal notranslate"><span class="pre">atom/utils/envs.py</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#additional-environment-variables-used-outside-envs-py">8.2 Additional Environment Variables (Used Outside <code class="docutils literal notranslate"><span class="pre">envs.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#decision-tree-choosing-a-compilation-level">9. Decision Tree – Choosing a Compilation Level</a></li>
<li class="toctree-l2"><a class="reference internal" href="#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_support_guide.html">ATOM Model Support Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#supported-model-architectures">1. Supported Model Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#model-architecture-details">2. Model Architecture Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#qwen3-qwen3forcausallm">Qwen3 (<code class="docutils literal notranslate"><span class="pre">Qwen3ForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#qwen3-moe-qwen3moeforcausallm">Qwen3-MoE (<code class="docutils literal notranslate"><span class="pre">Qwen3MoeForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#llama-llamaforcausallm">Llama (<code class="docutils literal notranslate"><span class="pre">LlamaForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#mixtral-mixtralforcausallm">Mixtral (<code class="docutils literal notranslate"><span class="pre">MixtralForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#deepseek-v2-v3-deepseekv2forcausallm">DeepSeek V2/V3 (<code class="docutils literal notranslate"><span class="pre">DeepseekV2ForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#deepseek-mtp-deepseekmtp">DeepSeek MTP (<code class="docutils literal notranslate"><span class="pre">DeepSeekMTP</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#gpt-oss-gptossforcausallm">GPT-OSS (<code class="docutils literal notranslate"><span class="pre">GptOssForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#glm4-moe-glm4moeforcausallm">GLM4-MoE (<code class="docutils literal notranslate"><span class="pre">Glm4MoeForCausalLM</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#weight-loading">3. Weight Loading</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#function-signature">Function Signature</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#loading-flow">Loading Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#layers-beyond-num-hidden-layers">Layers Beyond <code class="docutils literal notranslate"><span class="pre">num_hidden_layers</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#adding-a-new-model">4. Adding a New Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#step-1-create-the-model-file">Step 1: Create the Model File</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#step-2-implement-layer-classes">Step 2: Implement Layer Classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#step-3-implement-the-model-and-causallm-classes">Step 3: Implement the Model and CausalLM Classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#step-4-register-the-model">Step 4: Register the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#step-5-handle-weight-loading">Step 5: Handle Weight Loading</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#model-specific-optimizations">5. Model-Specific Optimizations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#llama-fused-rmsnorm-quant-and-silu-mul-quant">Llama: Fused RMSNorm+Quant and SiLU+Mul+Quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#deepseek-v2-v3-mla-fused-input-norm-qk-norm-fusion">DeepSeek V2/V3: MLA + Fused Input Norm + QK Norm Fusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#qwen3-moe-qk-norm-rope-cache-quant-fusion">Qwen3-MoE: QK Norm + RoPE + Cache + Quant Fusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_support_guide.html#mtp-deepseek-multi-token-prediction">MTP: DeepSeek Multi-Token Prediction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_support_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_ops_guide.html">ATOM Model Operations Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#aiter-integration-overview">1. AITER Integration Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#aiter-kernel-mapping-table">AITER Kernel Mapping Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#linear-operations">2. Linear Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#class-hierarchy">2.1 Class Hierarchy</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#quantization-dispatch">2.2 Quantization Dispatch</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#tensor-parallel-sharding">2.3 Tensor Parallel Sharding</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#weight-processing">2.4 Weight Processing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#attention-operations">3. Attention Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#base-attention-base-attention-py">3.1 Base: <code class="docutils literal notranslate"><span class="pre">Attention</span></code> (<code class="docutils literal notranslate"><span class="pre">base_attention.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#multi-head-attention-attention-mha-py">3.2 Multi-Head Attention (<code class="docutils literal notranslate"><span class="pre">attention_mha.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#multi-head-latent-attention-attention-mla-py">3.3 Multi-head Latent Attention (<code class="docutils literal notranslate"><span class="pre">attention_mla.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#backend-abstraction-attentions-backends-py">3.4 Backend Abstraction (<code class="docutils literal notranslate"><span class="pre">attentions/backends.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#kv-cache-operations">3.5 KV Cache Operations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#mixture-of-experts-moe">4. Mixture of Experts (MoE)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#fusedmoe-class-moe-py">4.1 <code class="docutils literal notranslate"><span class="pre">FusedMoE</span></code> Class (<code class="docutils literal notranslate"><span class="pre">moe.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#quantization-methods">4.2 Quantization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#topk-routing-topk-py">4.3 TopK Routing (<code class="docutils literal notranslate"><span class="pre">topK.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#fusedmoeparallelconfig">4.4 <code class="docutils literal notranslate"><span class="pre">FusedMoEParallelConfig</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#mori-integration-fused-moe-mori-prepare-finalize-py">4.5 MORI Integration (<code class="docutils literal notranslate"><span class="pre">fused_moe/mori_prepare_finalize.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#moe-quantization-config-fused-moe-config-py">4.6 MoE Quantization Config (<code class="docutils literal notranslate"><span class="pre">fused_moe/config.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#triton-moe-fallback-fused-moe-triton-py">4.7 Triton MoE Fallback (<code class="docutils literal notranslate"><span class="pre">fused_moe_triton.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#normalization">5. Normalization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#rmsnorm-layernorm-py">5.1 <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code> (<code class="docutils literal notranslate"><span class="pre">layernorm.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#layernorm-layernorm-py">5.2 <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> (<code class="docutils literal notranslate"><span class="pre">layernorm.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#activation-functions">6. Activation Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#siluandmul-activation-py">6.1 <code class="docutils literal notranslate"><span class="pre">SiluAndMul</span></code> (<code class="docutils literal notranslate"><span class="pre">activation.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#embedding-output-head">7. Embedding &amp; Output Head</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#vocabparallelembedding-embed-head-py">7.1 <code class="docutils literal notranslate"><span class="pre">VocabParallelEmbedding</span></code> (<code class="docutils literal notranslate"><span class="pre">embed_head.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#parallellmhead-embed-head-py">7.2 <code class="docutils literal notranslate"><span class="pre">ParallelLMHead</span></code> (<code class="docutils literal notranslate"><span class="pre">embed_head.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#rotary-position-embedding-rope">8. Rotary Position Embedding (RoPE)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#rotaryembedding-rotary-embedding-py">8.1 <code class="docutils literal notranslate"><span class="pre">RotaryEmbedding</span></code> (<code class="docutils literal notranslate"><span class="pre">rotary_embedding.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#get-rope-factory">8.2 <code class="docutils literal notranslate"><span class="pre">get_rope()</span></code> Factory</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#integration-in-attention">8.3 Integration in Attention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#sampling">9. Sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#sampler-sampler-py">9.1 <code class="docutils literal notranslate"><span class="pre">Sampler</span></code> (<code class="docutils literal notranslate"><span class="pre">sampler.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#rejectionsampler-rejection-sampler-py">9.2 <code class="docutils literal notranslate"><span class="pre">RejectionSampler</span></code> (<code class="docutils literal notranslate"><span class="pre">rejection_sampler.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#fused-kernel-chains">10. Fused Kernel Chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#source-files">Source Files</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-model-ops"><code class="docutils literal notranslate"><span class="pre">atom/model_ops/</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-model-ops-attentions"><code class="docutils literal notranslate"><span class="pre">atom/model_ops/attentions/</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-model-ops-fused-moe"><code class="docutils literal notranslate"><span class="pre">atom/model_ops/fused_moe/</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-utils"><code class="docutils literal notranslate"><span class="pre">atom/utils/</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="scheduling_kv_cache_guide.html">ATOM Scheduling &amp; KV Cache Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduling-algorithm">1. Scheduling Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduler-initialization">1.1 Scheduler Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#schedule-flow">1.2 Schedule Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#delay-factor">1.3 Delay Factor</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#preemption">1.4 Preemption</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduledbatch-structure">2. ScheduledBatch Structure</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#constructor-signature">2.1 Constructor Signature</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#fields">2.2 Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduledbatchoutput">2.3 ScheduledBatchOutput</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#block-manager">3. Block Manager</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#block-class">3.1 Block Class</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#blockmanager-initialization">3.2 BlockManager Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#allocation-allocate">3.3 Allocation (<code class="docutils literal notranslate"><span class="pre">allocate</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#deallocation-deallocate">3.4 Deallocation (<code class="docutils literal notranslate"><span class="pre">deallocate</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#can-allocate-and-can-append-checks">3.5 Can-Allocate and Can-Append Checks</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#may-append-decode-extension">3.6 May-Append (Decode Extension)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#prefix-caching">4. Prefix Caching</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#hash-function">4.1 Hash Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#hash-chaining">4.2 Hash Chaining</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#cache-lookup-during-allocation">4.3 Cache Lookup During Allocation</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#reference-counting">4.4 Reference Counting</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#enabling-prefix-caching">4.5 Enabling Prefix Caching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#postprocessing">5. Postprocessing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#signature">5.1 Signature</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#token-appending">5.2 Token Appending</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#stop-condition-checking">5.3 Stop Condition Checking</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#stream-output">5.4 Stream Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequence-cleanup">5.5 Sequence Cleanup</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#placeholder-insertion">5.6 Placeholder Insertion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#speculative-decoding-integration">6. Speculative Decoding Integration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduler-tracking">6.1 Scheduler Tracking</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#draft-tokens-in-scheduling">6.2 Draft Tokens in Scheduling</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#acceptance-statistics">6.3 Acceptance Statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#draft-token-storage-on-sequences">6.4 Draft Token Storage on Sequences</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequence-management">7. Sequence Management</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#constructor">7.1 Constructor</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#core-fields">7.2 Core Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#timing-fields">7.3 Timing Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#computed-properties">7.4 Computed Properties</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#num-tokens-setter">7.5 num_tokens Setter</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#lifecycle">7.6 Lifecycle</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequencestatus-enum">7.7 SequenceStatus Enum</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequencetype-enum">7.8 SequenceType Enum</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_guide.html">ATOM Distributed Inference Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#tensor-parallelism-tp">1. Tensor Parallelism (TP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#weight-sharding">Weight Sharding</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#process-group-initialization">Process Group Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#allreduce">AllReduce</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#configuration">Configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#data-parallelism-dp">2. Data Parallelism (DP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#architecture">Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#dp-process-group-initialization">DP Process Group Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#synchronized-busy-loop">Synchronized Busy Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#dummy-batch-execution">Dummy Batch Execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#device-assignment">Device Assignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#dpmetadata">DPMetadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#coremanager-dp-orchestration">CoreManager (DP Orchestration)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#id1">Configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#expert-parallelism-ep">3. Expert Parallelism (EP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#fusedmoeparallelconfig">FusedMoEParallelConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#expert-distribution">Expert Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#mori-communication">MORI Communication</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#id2">Configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#environment-variables">4. Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#multi-gpu-deployment-examples">5. Multi-GPU Deployment Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#deepseek-r1-on-8-gpus-tp8">DeepSeek-R1 on 8 GPUs (TP8)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#qwen3-235b-a22b-on-8-gpus-tp8-ep">Qwen3-235B-A22B on 8 GPUs (TP8 + EP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#kimi-k2-thinking-on-4-gpus-tp4">Kimi-K2-Thinking on 4 GPUs (TP4)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#combined-parallelism-strategies">6. Combined Parallelism Strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#tp-only-dense-models">TP Only (Dense Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#tp-ep-moe-models">TP + EP (MoE Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#tp-dp-dense-throughput">TP + DP (Dense Throughput)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#tp-dp-ep-moe-throughput">TP + DP + EP (MoE Throughput)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#dp-attention-mode">DP Attention Mode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="compilation_cudagraph_guide.html">ATOM Compilation &amp; CUDA Graphs Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#compilation-levels">1. Compilation Levels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-0-no-compilation">Level 0 – NO_COMPILATION</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-1-dynamo-as-is">Level 1 – DYNAMO_AS_IS</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-2-dynamo-once">Level 2 – DYNAMO_ONCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-3-piecewise-production-default">Level 3 – PIECEWISE (Production Default)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#cuda-graph-modes">2. CUDA Graph Modes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#none-value-0">NONE (value: 0)</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#piecewise-value-1">PIECEWISE (value: 1)</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#full-value-2">FULL (value: 2)</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#full-decode-only-value-full-none">FULL_DECODE_ONLY (value: (FULL, NONE))</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#full-and-piecewise-value-full-piecewise">FULL_AND_PIECEWISE (value: (FULL, PIECEWISE))</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#helper-methods">Helper Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#cuda-graph-capture">3. CUDA Graph Capture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#capture-flow">Capture Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#graph-keying">Graph Keying</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#graph-pool-sharing">Graph Pool Sharing</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#default-capture-sizes">Default Capture Sizes</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#graph-replay-in-run-model">Graph Replay in run_model()</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#piecewise-compilation">4. Piecewise Compilation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#splitting-operations">Splitting Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#compilation-pipeline">Compilation Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#cache-management">Cache Management</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#forward-context-stateless-dispatch">5. Forward Context &amp; Stateless Dispatch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#forwardcontext-fields">ForwardContext Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#lifecycle">Lifecycle</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#context-dataclass">Context Dataclass</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#integration-with-cuda-graphs">Integration with CUDA Graphs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#compiler-backend">6. Compiler Backend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#compilermanager">CompilerManager</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#compilerinterface">CompilerInterface</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#inductoradaptor">InductorAdaptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#inductorstandaloneadaptor">InductorStandaloneAdaptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#vllmbackend">VllmBackend</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#support-torch-compile-decorator">&#64;support_torch_compile Decorator</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#custom-op-registration">Custom Op Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#configuration-options">7. Configuration Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#decision-tree">8. Decision Tree</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#common-configurations">Common Configurations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="serving_benchmarking_guide.html">ATOM Serving &amp; Benchmarking Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#openai-compatible-server">1. OpenAI-Compatible Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#endpoints">1.1 Endpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#request-models">1.2 Request Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#response-models">1.3 Response Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#server-startup">1.4 Server Startup</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#example-curl">1.5 Example: curl</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#programmatic-api-llmengine">2. Programmatic API (LLMEngine)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#initialization">2.1 Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#samplingparams">2.2 SamplingParams</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#core-methods">2.3 Core Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#synchronous-generation-example">2.4 Synchronous Generation Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#asynchronous-streaming-usage">2.5 Asynchronous / Streaming Usage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#simple-inference">3. Simple Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#usage">3.1 Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#what-it-does">3.2 What It Does</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#benchmarking">4. Benchmarking</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#metrics">4.1 Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#key-cli-arguments">4.2 Key CLI Arguments</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#backend-request-functions">4.3 Backend Request Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#full-benchmark-example">4.4 Full Benchmark Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#profiling">5. Profiling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#configuration">5.1 Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#online-profiling-http">5.2 Online Profiling (HTTP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#programmatic-profiling">5.3 Programmatic Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#offline-profiling-script">5.4 Offline Profiling Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#profiling-during-benchmarks">5.5 Profiling During Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#speculative-decoding-mtp">6. Speculative Decoding (MTP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#architecture">6.1 Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#id1">6.2 Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#mtp-statistics">6.3 MTP Statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#how-rejection-sampling-works">6.4 How Rejection Sampling Works</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#deployment-examples">7. Deployment Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#single-gpu">7.1 Single-GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#multi-gpu-with-tensor-parallelism">7.2 Multi-GPU with Tensor Parallelism</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#docker-deployment">7.3 Docker Deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#engine-cli-arguments-engineargs">7.4 Engine CLI Arguments (EngineArgs)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#accuracy-validation">8. Accuracy Validation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#setup">8.1 Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#run-evaluation">8.2 Run Evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#source-files">Source Files</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api/serving.html">Serving API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#llmengine-class">LLMEngine Class</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/serving.html#methods">Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/serving.html#generate">generate()</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#samplingparams">SamplingParams</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#return-values">Return Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#example">Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/models.html">Supported Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#llama-models">Llama Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#gpt-models">GPT Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#mixtral">Mixtral</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#other-architectures">Other Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#performance-by-model-size">Performance by Model Size</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#quantization">Quantization</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #C00000" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ATOM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">ATOM Configuration Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/configuration_guide.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="atom-configuration-guide">
<h1>ATOM Configuration Guide<a class="headerlink" href="#atom-configuration-guide" title="Link to this heading"></a></h1>
<p>ATOM (AiTer Optimized Model) is AMD’s lightweight LLM inference engine built on
<a class="reference external" href="https://github.com/ROCm/aiter">AITER</a> kernels for ROCm/HIP GPUs. This guide
documents every configuration class, CLI flag, and environment variable that
controls ATOM’s runtime behaviour.</p>
<hr class="docutils" />
<section id="quick-reference">
<h2>Quick Reference<a class="headerlink" href="#quick-reference" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Config Class</p></th>
<th class="head"><p>Primary Purpose</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Config</span></code></p></td>
<td><p>Master dataclass – model path, memory, TP size, scheduler limits, KV cache, profiler, and references to all sub-configs</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code></p></td>
<td><p>Compilation level (0-3), CUDA graph capture sizes, piecewise splitting ops, inductor settings</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">CompilationLevel</span></code></p></td>
<td><p>Integer constants for the four compilation levels</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CUDAGraphMode</span></code></p></td>
<td><p>Enum controlling how CUDA graphs are captured (none / piecewise / full / hybrid)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code></p></td>
<td><p>Quantization type, dtype, dynamic flag, method, excluded layers</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code></p></td>
<td><p>Data-parallel size, rank, master IP/port</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SpeculativeConfig</span></code></p></td>
<td><p>Speculative decoding method, draft model, number of speculative tokens</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">KVCacheConfig</span></code> / <code class="docutils literal notranslate"><span class="pre">KVCacheTensor</span></code></p></td>
<td><p>Per-layer KV cache tensor descriptors (k/v caches and scales)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SamplingParams</span></code></p></td>
<td><p>Temperature, max tokens, stop strings, ignore-EOS flag</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">EngineArgs</span></code></p></td>
<td><p>CLI argument parser that builds a <code class="docutils literal notranslate"><span class="pre">Config</span></code> for <code class="docutils literal notranslate"><span class="pre">LLMEngine</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="master-configuration-config">
<h2>1. Master Configuration (<code class="docutils literal notranslate"><span class="pre">Config</span></code>)<a class="headerlink" href="#master-configuration-config" title="Link to this heading"></a></h2>
<p>Defined in <code class="docutils literal notranslate"><span class="pre">atom/config.py</span></code>. The root dataclass that the engine consumes.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">model</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><em>(required)</em></p></td>
<td><p>HuggingFace model name or local path</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">trust_remote_code</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Trust remote code when loading the model from HuggingFace</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">16384</span></code></p></td>
<td><p>Maximum number of tokens batched together per scheduler step</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">scheduler_delay_factor</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0.0</span></code></p></td>
<td><p>Multiplicative delay (factor x previous prompt latency) before scheduling the next prompt</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">max_num_seqs</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">512</span></code></p></td>
<td><p>Maximum number of sequences batched together</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">max_model_len</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Maximum context length; defaults to <code class="docutils literal notranslate"><span class="pre">hf_config.max_position_embeddings</span></code> (capped by it when set)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">gpu_memory_utilization</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0.9</span></code></p></td>
<td><p>Fraction of GPU memory available for KV cache and weights (0.0 – 1.0)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">tensor_parallel_size</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Number of tensor-parallel GPUs (1 – 8)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">enforce_eager</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Disable compilation and CUDA graphs; run in eager mode</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">parallel_config</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ParallelConfig()</span></code></p></td>
<td><p>Data-parallel configuration (see Section 4)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">kv_cache_block_size</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">16</span></code></p></td>
<td><p>Block size for paged KV cache; must be a multiple of 16 or exactly 1</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">num_kvcache_blocks</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-1</span></code></p></td>
<td><p>Number of KV cache blocks (<code class="docutils literal notranslate"><span class="pre">-1</span></code> = auto)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">kv_cache_dtype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;bf16&quot;</span></code></p></td>
<td><p>KV cache data type (<code class="docutils literal notranslate"><span class="pre">&quot;bf16&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;fp8&quot;</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">enable_prefix_caching</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Enable prefix caching to reuse KV blocks across requests sharing the same prefix</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">port</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">8006</span></code></p></td>
<td><p>Engine internal communication port</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">torch_profiler_dir</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">os.getenv(&quot;ATOM_TORCH_PROFILER_DIR&quot;,</span> <span class="pre">None)</span></code></p></td>
<td><p>Directory for saving PyTorch profiler traces; creates the directory if it does not exist</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">compilation_config</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CompilationConfig()</span></code></p></td>
<td><p>Compilation and CUDA graph settings (see Section 2)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">quant_config</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">QuantizationConfig()</span></code></p></td>
<td><p>Quantization settings; auto-detected from HuggingFace config during <code class="docutils literal notranslate"><span class="pre">__post_init__</span></code> (see Section 3)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">asyncio_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Enable asyncio-based engine loop</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">load_dummy</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Skip loading model weights (for benchmarking / testing)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">enable_expert_parallel</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Enable Expert Parallelism for MoE models</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">master_addr</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;127.0.0.1&quot;</span></code></p></td>
<td><p>Master address for distributed communication</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">graph_bs</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[list[int]]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Explicit list of batch sizes for CUDA graph capture; derived from <code class="docutils literal notranslate"><span class="pre">compilation_config</span></code> during init</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">enable_dp_attention</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Enable data-parallel attention</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code></p></td>
<td><p><em>(computed)</em></p></td>
<td><p>Inferred from <code class="docutils literal notranslate"><span class="pre">hf_config.torch_dtype</span></code>; falls back to <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">speculative_config</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[SpeculativeConfig]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Speculative decoding configuration (see Section 5)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">bos_token_id</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-1</span></code></p></td>
<td><p>Beginning-of-sequence token ID (<code class="docutils literal notranslate"><span class="pre">-1</span></code> = use model default)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">eos_token_id</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-1</span></code></p></td>
<td><p>End-of-sequence token ID (<code class="docutils literal notranslate"><span class="pre">-1</span></code> = use model default)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">stop_token_ids</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">list[int]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[]</span></code></p></td>
<td><p>Additional stop token IDs; populated from <code class="docutils literal notranslate"><span class="pre">GenerationConfig.eos_token_id</span></code> during init</p></td>
</tr>
</tbody>
</table>
<p><strong>Auto-derived fields</strong> (set in <code class="docutils literal notranslate"><span class="pre">__post_init__</span></code>, not user-supplied):</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">hf_config</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></p></td>
<td><p>Loaded automatically via <code class="docutils literal notranslate"><span class="pre">get_hf_config(model)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">generation_config</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GenerationConfig</span></code></p></td>
<td><p>Loaded automatically via <code class="docutils literal notranslate"><span class="pre">get_generation_config(model)</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="compilation-configuration-compilationconfig">
<h2>2. Compilation Configuration (<code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code>)<a class="headerlink" href="#compilation-configuration-compilationconfig" title="Link to this heading"></a></h2>
<p>Defined in <code class="docutils literal notranslate"><span class="pre">atom/config.py</span></code>. Controls torch.compile and CUDA graph behaviour.</p>
<section id="compilation-levels-compilationlevel">
<h3>2.1 Compilation Levels (<code class="docutils literal notranslate"><span class="pre">CompilationLevel</span></code>)<a class="headerlink" href="#compilation-levels-compilationlevel" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Constant</p></th>
<th class="head"><p>Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">NO_COMPILATION</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p>No compilation – pure eager execution</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">DYNAMO_AS_IS</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Use torch.compile / TorchDynamo as-is</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">DYNAMO_ONCE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">2</span></code></p></td>
<td><p>TorchDynamo with a single compilation pass</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">PIECEWISE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">3</span></code></p></td>
<td><p>Piecewise compilation with CUDA graph capture (recommended for production)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="compilationconfig-fields">
<h3>2.2 <code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code> Fields<a class="headerlink" href="#compilationconfig-fields" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">level</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p>Compilation level (see table above); must be 0 – 3</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">use_cudagraph</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code></p></td>
<td><p>Whether to use CUDA graphs</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">cudagraph_capture_sizes</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[list[int]]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Explicit list of batch sizes for CUDA graph capture; overrides <code class="docutils literal notranslate"><span class="pre">cuda_graph_sizes</span></code> when set</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cuda_graph_sizes</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">list[int]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[]</span></code> (post-init: <code class="docutils literal notranslate"><span class="pre">[512]</span></code>)</p></td>
<td><p>CUDA graph sizing strategy: 1 value generates <code class="docutils literal notranslate"><span class="pre">[1,2,4,8]</span> <span class="pre">+</span> <span class="pre">range(16,</span> <span class="pre">N+1,</span> <span class="pre">16)</span></code>; multiple values used as-is; empty defaults to <code class="docutils literal notranslate"><span class="pre">[512]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">debug_dump_path</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code></p></td>
<td><p>Path to dump debug / compilation information</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cache_dir</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code></p></td>
<td><p>Directory for compilation caches</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">use_inductor</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code></p></td>
<td><p>Enable TorchInductor backend</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cudagraph_mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[CUDAGraphMode]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>CUDA graph capture mode (see below); set to <code class="docutils literal notranslate"><span class="pre">PIECEWISE</span></code> automatically at level 3</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">splitting_ops</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[list[str]]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Ops that split the graph into sub-graphs for piecewise compilation; auto-populated at level 3 with <code class="docutils literal notranslate"><span class="pre">[&quot;aiter.unified_attention_with_output&quot;,</span> <span class="pre">&quot;aiter.mla_attention&quot;]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cudagraph_copy_inputs</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Copy input tensors into internally managed buffers before CUDA graph replay; only effective in PIECEWISE mode</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">compile_sizes</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[list[Union[int,</span> <span class="pre">str]]]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Sizes to compile for inductor; accepts integers and the string <code class="docutils literal notranslate"><span class="pre">&quot;cudagraph_capture_sizes&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">inductor_compile_config</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">dict</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">{}</span></code></p></td>
<td><p>Additional configuration passed to the inductor backend</p></td>
</tr>
</tbody>
</table>
</section>
<section id="cuda-graph-mode-cudagraphmode">
<h3>2.3 CUDA Graph Mode (<code class="docutils literal notranslate"><span class="pre">CUDAGraphMode</span></code>)<a class="headerlink" href="#cuda-graph-mode-cudagraphmode" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Mode</p></th>
<th class="head"><p>Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">NONE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p>No CUDA graph capture</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">PIECEWISE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Piecewise CUDA graphs – attention ops stay outside the graph for flexibility (default at level 3)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">FULL</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">2</span></code></p></td>
<td><p>Full CUDA graph capture for all batches; best for small models / short prompts</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">FULL_DECODE_ONLY</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(FULL,</span> <span class="pre">NONE)</span></code></p></td>
<td><p>Full CUDA graphs for decode batches only; mixed prefill-decode runs without graphs (useful in P/D setups)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">FULL_AND_PIECEWISE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(FULL,</span> <span class="pre">PIECEWISE)</span></code></p></td>
<td><p>Full graphs for decode, piecewise for prefill/mixed – most performant mode for most models</p></td>
</tr>
</tbody>
</table>
<p>Helper methods on <code class="docutils literal notranslate"><span class="pre">CUDAGraphMode</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">decode_mode()</span></code> – returns the mode used for pure decode batches.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mixed_mode()</span></code> – returns the mode used for mixed prefill-decode batches.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">requires_piecewise_compilation()</span></code> – whether the mode needs piecewise compilation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">has_full_cudagraphs()</span></code> – whether the mode includes full CUDA graph capture.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">separate_routine()</span></code> – whether decode and mixed batches use different routines.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="quantization-configuration-quantizationconfig">
<h2>3. Quantization Configuration (<code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code>)<a class="headerlink" href="#quantization-configuration-quantizationconfig" title="Link to this heading"></a></h2>
<p>Defined in <code class="docutils literal notranslate"><span class="pre">atom/config.py</span></code>. Extends <code class="docutils literal notranslate"><span class="pre">dict</span></code> so fields are stored and accessed as
dictionary keys (e.g., <code class="docutils literal notranslate"><span class="pre">config[&quot;quant_type&quot;]</span></code>).</p>
<section id="quantizationconfig-fields">
<h3>3.1 <code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code> Fields<a class="headerlink" href="#quantizationconfig-fields" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">quant_type</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">QuantType</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">QuantType.No</span></code></p></td>
<td><p>Quantization granularity (see below)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">quant_dtype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code></p></td>
<td><p>Data type for quantized weights</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">is_dynamic</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code></p></td>
<td><p>Use dynamic quantization (scales computed at runtime)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">quant_name</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code></p></td>
<td><p>Human-readable name for the quantization scheme</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">quant_method</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[str]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Quantization method from HuggingFace config (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;compressed-tensors&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;quark&quot;</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">exclude_layers</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[list[str]]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[]</span></code></p></td>
<td><p>Layer names excluded from quantization</p></td>
</tr>
</tbody>
</table>
</section>
<section id="quanttype-values-from-aiter">
<h3>3.2 <code class="docutils literal notranslate"><span class="pre">QuantType</span></code> Values (from AITER)<a class="headerlink" href="#quanttype-values-from-aiter" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">QuantType.No</span></code></p></td>
<td><p>No quantization</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">QuantType.per_Token</span></code></p></td>
<td><p>Per-token / per-channel quantization</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">QuantType.per_1x128</span></code></p></td>
<td><p>Block quantization with group size 128</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">QuantType.per_1x32</span></code></p></td>
<td><p>Block quantization with group size 32</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">QuantType.per_128x128</span></code></p></td>
<td><p>Large 2D block quantization (remapped to <code class="docutils literal notranslate"><span class="pre">per_1x128</span></code> in MoE kernels)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">QuantType.per_Tensor</span></code></p></td>
<td><p>Per-tensor quantization</p></td>
</tr>
</tbody>
</table>
</section>
<section id="supported-quantization-dtypes">
<h3>3.3 Supported Quantization Dtypes<a class="headerlink" href="#supported-quantization-dtypes" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Dtype</p></th>
<th class="head"><p>AITER Key</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP8 (E4M3)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;fp8&quot;</span></code></p></td>
<td><p>8-bit floating point</p></td>
</tr>
<tr class="row-odd"><td><p>MXFP4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;fp4x2&quot;</span></code></p></td>
<td><p>Microscaling FP4; forces <code class="docutils literal notranslate"><span class="pre">QuantType.per_1x32</span></code></p></td>
</tr>
<tr class="row-even"><td><p>INT8</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;i8&quot;</span></code></p></td>
<td><p>8-bit integer</p></td>
</tr>
<tr class="row-odd"><td><p>INT4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;i4x2&quot;</span></code></p></td>
<td><p>4-bit integer (packed)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="auto-detection-from-huggingface-get-quant-config">
<h3>3.4 Auto-Detection from HuggingFace (<code class="docutils literal notranslate"><span class="pre">get_quant_config</span></code>)<a class="headerlink" href="#auto-detection-from-huggingface-get-quant-config" title="Link to this heading"></a></h3>
<p>During <code class="docutils literal notranslate"><span class="pre">Config.__post_init__</span></code>, ATOM reads <code class="docutils literal notranslate"><span class="pre">hf_config.quantization_config</span></code> and
automatically determines <code class="docutils literal notranslate"><span class="pre">quant_type</span></code>, <code class="docutils literal notranslate"><span class="pre">quant_dtype</span></code>, and <code class="docutils literal notranslate"><span class="pre">is_dynamic</span></code>:</p>
<ol class="arabic simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">quantization_config</span></code> is absent, returns <code class="docutils literal notranslate"><span class="pre">QuantType.No</span></code> with <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code>.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">quant_method</span> <span class="pre">==</span> <span class="pre">&quot;compressed-tensors&quot;</span></code> or channel quantization is detected, sets <code class="docutils literal notranslate"><span class="pre">per_Token</span></code>.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">weight_block_size</span></code> or <code class="docutils literal notranslate"><span class="pre">group_size</span></code> is found: group size 128 maps to <code class="docutils literal notranslate"><span class="pre">per_1x128</span></code>, group size 32 maps to <code class="docutils literal notranslate"><span class="pre">per_1x32</span></code>.</p></li>
<li><p>Otherwise falls back to <code class="docutils literal notranslate"><span class="pre">per_Tensor</span></code>.</p></li>
<li><p>The dtype is parsed from fields like <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_dtype</span></code>, or <code class="docutils literal notranslate"><span class="pre">quant_method</span></code> looking for <code class="docutils literal notranslate"><span class="pre">fp8</span></code>, <code class="docutils literal notranslate"><span class="pre">fp4</span></code>, <code class="docutils literal notranslate"><span class="pre">mxfp4</span></code>, <code class="docutils literal notranslate"><span class="pre">int8</span></code>, <code class="docutils literal notranslate"><span class="pre">int4</span></code>, or <code class="docutils literal notranslate"><span class="pre">num_bits</span></code>.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">activation_scheme</span></code> is <code class="docutils literal notranslate"><span class="pre">&quot;static&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">is_dynamic</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>Excluded layers are read from the <code class="docutils literal notranslate"><span class="pre">&quot;ignore&quot;</span></code> key (compressed-tensors) or <code class="docutils literal notranslate"><span class="pre">&quot;exclude&quot;</span></code> key (quark).</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="parallel-configuration-parallelconfig">
<h2>4. Parallel Configuration (<code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code>)<a class="headerlink" href="#parallel-configuration-parallelconfig" title="Link to this heading"></a></h2>
<p>Defined in <code class="docutils literal notranslate"><span class="pre">atom/config.py</span></code>. Controls data parallelism. Environment variables
(Section 8) override defaults when set.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">data_parallel_size</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Number of data-parallel groups; overridden by <code class="docutils literal notranslate"><span class="pre">ATOM_DP_SIZE</span></code> env var</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">data_parallel_size_local</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Number of local data-parallel groups</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">data_parallel_rank</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p>Rank within the data-parallel group; overridden by <code class="docutils literal notranslate"><span class="pre">ATOM_DP_RANK</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">data_parallel_rank_local</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Local rank within the data-parallel group (SPMD mode); overridden by <code class="docutils literal notranslate"><span class="pre">ATOM_DP_RANK_LOCAL</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">data_parallel_master_port</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">29500</span></code></p></td>
<td><p>Port used by the data-parallel master for process group initialization</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">data_parallel_base_port</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">get_open_port()</span></code></p></td>
<td><p>Base port for data-parallel communication (dynamically assigned)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">data_parallel_master_ip</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;127.0.0.1&quot;</span></code></p></td>
<td><p>IP address of the data-parallel master</p></td>
</tr>
</tbody>
</table>
<p><strong>Computed property:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">world_size</span></code> – set during init, equals TP x PP.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">world_size_across_dp</span></code> – <code class="docutils literal notranslate"><span class="pre">world_size</span> <span class="pre">*</span> <span class="pre">data_parallel_size</span></code>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="speculative-decoding-configuration-speculativeconfig">
<h2>5. Speculative Decoding Configuration (<code class="docutils literal notranslate"><span class="pre">SpeculativeConfig</span></code>)<a class="headerlink" href="#speculative-decoding-configuration-speculativeconfig" title="Link to this heading"></a></h2>
<p>Defined in <code class="docutils literal notranslate"><span class="pre">atom/config.py</span></code>. Currently only the Multi-Token Prediction (MTP)
method with <code class="docutils literal notranslate"><span class="pre">num_speculative_tokens=1</span></code> is supported.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">method</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[str]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code></p></td>
<td><p>Speculative decoding method; currently only <code class="docutils literal notranslate"><span class="pre">&quot;mtp&quot;</span></code> is accepted</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">model</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[str]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Draft model name or path (typically the same as the target model for MTP)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">num_speculative_tokens</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Number of speculative tokens per iteration; <strong>must be <code class="docutils literal notranslate"><span class="pre">1</span></code></strong></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">draft_model_hf_config</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[PretrainedConfig]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>HuggingFace config for the draft model; auto-loaded from <code class="docutils literal notranslate"><span class="pre">model</span></code> when <code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
</tr>
</tbody>
</table>
<p><strong>Post-init behaviour:</strong></p>
<ul class="simple">
<li><p>Loads <code class="docutils literal notranslate"><span class="pre">draft_model_hf_config</span></code> from <code class="docutils literal notranslate"><span class="pre">model</span></code> if not provided.</p></li>
<li><p>For DeepSeek V3 / MTP models: overrides <code class="docutils literal notranslate"><span class="pre">model_type</span></code> to <code class="docutils literal notranslate"><span class="pre">&quot;deepseek_mtp&quot;</span></code>, sets <code class="docutils literal notranslate"><span class="pre">n_predict=1</span></code> and <code class="docutils literal notranslate"><span class="pre">num_nextn_predict_layers=1</span></code>, and switches architectures to <code class="docutils literal notranslate"><span class="pre">[&quot;DeepSeekMTPModel&quot;]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Config.__post_init__</span></code> raises <code class="docutils literal notranslate"><span class="pre">ValueError</span></code> if <code class="docutils literal notranslate"><span class="pre">num_speculative_tokens</span> <span class="pre">!=</span> <span class="pre">1</span></code>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="sampling-parameters-samplingparams">
<h2>6. Sampling Parameters (<code class="docutils literal notranslate"><span class="pre">SamplingParams</span></code>)<a class="headerlink" href="#sampling-parameters-samplingparams" title="Link to this heading"></a></h2>
<p>Defined in <code class="docutils literal notranslate"><span class="pre">atom/sampling_params.py</span></code>. Passed per-request to control generation.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1.0</span></code></p></td>
<td><p>Sampling temperature; lower values make output more deterministic</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">64</span></code></p></td>
<td><p>Maximum number of tokens to generate</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ignore_eos</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Continue generating past the EOS token</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">stop_strings</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional[list[str]]</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>List of strings that trigger generation to stop</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="cli-arguments-engineargs">
<h2>7. CLI Arguments (<code class="docutils literal notranslate"><span class="pre">EngineArgs</span></code>)<a class="headerlink" href="#cli-arguments-engineargs" title="Link to this heading"></a></h2>
<p>Defined in <code class="docutils literal notranslate"><span class="pre">atom/model_engine/arg_utils.py</span></code>. The <code class="docutils literal notranslate"><span class="pre">EngineArgs</span></code> dataclass exposes
all flags via <code class="docutils literal notranslate"><span class="pre">add_cli_args()</span></code> and converts them into a <code class="docutils literal notranslate"><span class="pre">Config</span></code> via
<code class="docutils literal notranslate"><span class="pre">create_engine()</span></code>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Flag</p></th>
<th class="head"><p>Short</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--model</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;Qwen/Qwen3-0.6B&quot;</span></code></p></td>
<td><p>Model name or path</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--trust-remote-code</span></code></p></td>
<td><p></p></td>
<td><p>flag</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Trust remote code when loading model</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--tensor-parallel-size</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-tp</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Tensor parallel size</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--data-parallel-size</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-dp</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Data parallel size</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--enforce-eager</span></code></p></td>
<td><p></p></td>
<td><p>flag</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Enforce eager mode execution</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--enable_prefix_caching</span></code></p></td>
<td><p></p></td>
<td><p>flag</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Enable prefix caching</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--port</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">8006</span></code></p></td>
<td><p>Engine internal port</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--kv_cache_dtype</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;bf16&quot;</span></code></p></td>
<td><p>KV cache dtype; choices: <code class="docutils literal notranslate"><span class="pre">bf16</span></code>, <code class="docutils literal notranslate"><span class="pre">fp8</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--block-size</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">16</span></code></p></td>
<td><p>KV cache block size (maps to <code class="docutils literal notranslate"><span class="pre">kv_cache_block_size</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--max-model-len</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Maximum model context length; defaults to <code class="docutils literal notranslate"><span class="pre">hf_config.max_position_embeddings</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--cudagraph-capture-sizes</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;[1,2,4,8,16,32,48,64,128,256]&quot;</span></code></p></td>
<td><p>CUDA graph capture sizes as a Python list string</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--level</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">3</span></code></p></td>
<td><p>Compilation level (0 – 3)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--load_dummy</span></code></p></td>
<td><p></p></td>
<td><p>flag</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Skip loading model weights</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--enable-expert-parallel</span></code></p></td>
<td><p></p></td>
<td><p>flag</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Enable Expert Parallelism (EP MoE)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--torch-profiler-dir</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Directory for torch profiler traces</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--enable-dp-attention</span></code></p></td>
<td><p></p></td>
<td><p>flag</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Enable DP attention</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--method</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Speculative method; choices: <code class="docutils literal notranslate"><span class="pre">mtp</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--num-speculative-tokens</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Number of speculative tokens per iteration</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--max-num-batched-tokens</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">16384</span></code></p></td>
<td><p>Maximum number of tokens to batch in the async engine</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--max-num-seqs</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">512</span></code></p></td>
<td><p>Maximum number of sequences to batch together</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--gpu-memory-utilization</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0.9</span></code></p></td>
<td><p>Fraction of GPU memory to use (0.0 – 1.0)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--scheduler-delay-factor</span></code></p></td>
<td><p></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0.0</span></code></p></td>
<td><p>Delay factor multiplied by previous prompt latency before scheduling next prompt</p></td>
</tr>
</tbody>
</table>
<p><strong>Example:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>atom.entrypoint<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>deepseek-ai/DeepSeek-R1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tensor-parallel-size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--level<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cudagraph-capture-sizes<span class="w"> </span><span class="s2">&quot;[1,2,4,8,16,32,64,128,256]&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--kv_cache_dtype<span class="w"> </span>fp8<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.92<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-num-seqs<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="environment-variables">
<h2>8. Environment Variables<a class="headerlink" href="#environment-variables" title="Link to this heading"></a></h2>
<section id="variables-registered-in-atom-utils-envs-py">
<h3>8.1 Variables Registered in <code class="docutils literal notranslate"><span class="pre">atom/utils/envs.py</span></code><a class="headerlink" href="#variables-registered-in-atom-utils-envs-py" title="Link to this heading"></a></h3>
<p>All variables use lazy evaluation. Boolean variables treat <code class="docutils literal notranslate"><span class="pre">&quot;1&quot;</span></code> as <code class="docutils literal notranslate"><span class="pre">True</span></code> and
anything else (including unset) as <code class="docutils literal notranslate"><span class="pre">False</span></code>, unless noted otherwise.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Variable</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_RANK</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p>Data-parallel rank of this process</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_RANK_LOCAL</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p>Local data-parallel rank (for SPMD mode)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_SIZE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Total number of data-parallel groups</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_MASTER_IP</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;127.0.0.1&quot;</span></code></p></td>
<td><p>IP address of the data-parallel master</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_DP_MASTER_PORT</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">29500</span></code></p></td>
<td><p>Port of the data-parallel master</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_ENFORCE_EAGER</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Force eager mode globally (also set programmatically by <code class="docutils literal notranslate"><span class="pre">set_current_atom_config</span></code>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_QK_NORM_ROPE_CACHE_QUANT_FUSION</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Enable QK-norm + RoPE + cache + quant fusion; enable for Qwen3-MoE models</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_USE_TRITON_GEMM</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Use Triton-based GEMM kernels instead of default backends</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_USE_TRITON_MXFP4_BMM</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Use Triton-based MXFP4 batched matrix multiply</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_DS_INPUT_RMSNORM_QUANT_FUSION</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code></p></td>
<td><p>Enable fused input RMSNorm + quantization for DeepSeek models</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_DS_QKNORM_QUANT_FUSION</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code></p></td>
<td><p>Enable fused QK-norm + quantization for DeepSeek models</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_ALLREDUCE_RMSNORM_FUSION</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code></p></td>
<td><p>Enable fused all-reduce + RMSNorm kernel</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_LLAMA_ENABLE_AITER_TRITON_FUSED_RMSNORM_QUANT</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code></p></td>
<td><p>Enable AITER Triton fused RMSNorm + quantization for LLaMA models</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_LLAMA_ENABLE_AITER_TRITON_FUSED_SILU_MUL_QUANT</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code></p></td>
<td><p>Enable AITER Triton fused SiLU + multiply + quantization for LLaMA models</p></td>
</tr>
</tbody>
</table>
</section>
<section id="additional-environment-variables-used-outside-envs-py">
<h3>8.2 Additional Environment Variables (Used Outside <code class="docutils literal notranslate"><span class="pre">envs.py</span></code>)<a class="headerlink" href="#additional-environment-variables-used-outside-envs-py" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Variable</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Where Used</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_TORCH_PROFILER_DIR</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom/config.py</span></code> (<code class="docutils literal notranslate"><span class="pre">Config.torch_profiler_dir</span></code>)</p></td>
<td><p>Directory for PyTorch profiler output; sets the default for <code class="docutils literal notranslate"><span class="pre">Config.torch_profiler_dir</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ATOM_PROFILER_MORE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom/model_engine/model_runner.py</span></code></p></td>
<td><p>Set to <code class="docutils literal notranslate"><span class="pre">&quot;1&quot;</span></code> to enable detailed profiling (<code class="docutils literal notranslate"><span class="pre">record_shapes</span></code>, <code class="docutils literal notranslate"><span class="pre">with_stack</span></code>, <code class="docutils literal notranslate"><span class="pre">profile_memory</span></code>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HF_TOKEN</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom/config.py</span></code> (<code class="docutils literal notranslate"><span class="pre">get_hf_config</span></code>)</p></td>
<td><p>HuggingFace authentication token for gated model downloads</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<hr class="docutils" />
<section id="decision-tree-choosing-a-compilation-level">
<h2>9. Decision Tree – Choosing a Compilation Level<a class="headerlink" href="#decision-tree-choosing-a-compilation-level" title="Link to this heading"></a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Start
  |
  v
Is this a debugging / development run?
  |-- Yes --&gt; Level 0 (NO_COMPILATION) or --enforce-eager
  |
  v
Do you need torch.compile but no graph splitting?
  |-- Yes, one-shot compile --&gt; Level 2 (DYNAMO_ONCE)
  |-- Yes, keep Dynamo default --&gt; Level 1 (DYNAMO_AS_IS)
  |
  v
Production inference on ROCm/HIP GPU?
  |-- Yes --&gt; Level 3 (PIECEWISE) [default in EngineArgs]
              - Auto-sets CUDAGraphMode.PIECEWISE
              - Auto-populates splitting_ops for attention ops
              - Pair with --cudagraph-capture-sizes for your batch profile
  |
  v
Need maximum decode throughput?
  |-- Yes --&gt; Level 3 + set cudagraph_mode to FULL_AND_PIECEWISE
              (full graphs for decode, piecewise for prefill)
</pre></div>
</div>
<p><strong>Rules of thumb:</strong></p>
<ul class="simple">
<li><p><strong>Level 3</strong> is the default for <code class="docutils literal notranslate"><span class="pre">EngineArgs</span></code> and is recommended for most
production workloads.</p></li>
<li><p><strong>Level 0</strong> / <code class="docutils literal notranslate"><span class="pre">--enforce-eager</span></code> is useful for debugging, profiling, or when
CUDA graphs are incompatible with your model.</p></li>
<li><p>Match <code class="docutils literal notranslate"><span class="pre">--cudagraph-capture-sizes</span></code> to your expected batch sizes for optimal
memory usage and launch latency.</p></li>
<li><p>When using <code class="docutils literal notranslate"><span class="pre">--enable-dp-attention</span></code> or Expert Parallelism (<code class="docutils literal notranslate"><span class="pre">--enable-expert-parallel</span></code>),
level 3 is still recommended.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="source-files">
<h2>Source Files<a class="headerlink" href="#source-files" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>File</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/config.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Config</span></code>, <code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">CompilationLevel</span></code>, <code class="docutils literal notranslate"><span class="pre">CUDAGraphMode</span></code>, <code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">SpeculativeConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">KVCacheTensor</span></code>, <code class="docutils literal notranslate"><span class="pre">KVCacheConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">get_quant_config</span></code>, <code class="docutils literal notranslate"><span class="pre">get_hf_config</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/utils/envs.py</span></code></p></td>
<td><p>All <code class="docutils literal notranslate"><span class="pre">ATOM_*</span></code> environment variable definitions with lazy evaluation</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_engine/arg_utils.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">EngineArgs</span></code> dataclass and CLI argument parser</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/sampling_params.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SamplingParams</span></code> dataclass</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_engine/model_runner.py</span></code></p></td>
<td><p>Uses <code class="docutils literal notranslate"><span class="pre">ATOM_PROFILER_MORE</span></code> and <code class="docutils literal notranslate"><span class="pre">ATOM_TORCH_PROFILER_DIR</span></code> for profiling</p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="architecture_guide.html" class="btn btn-neutral float-left" title="ATOM Architecture Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="model_support_guide.html" class="btn btn-neutral float-right" title="ATOM Model Support Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, AMD.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>