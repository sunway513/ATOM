

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ATOM Model Support Guide &mdash; ATOM 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=9edc463e" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=01f34227"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="ATOM Model Operations Guide" href="model_ops_guide.html" />
    <link rel="prev" title="ATOM Configuration Guide" href="configuration_guide.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #C00000" >

          
          
          <a href="index.html" class="icon icon-home">
            ATOM
              <img src="_static/atom_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#installation-methods">Installation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="installation.html#from-source">From Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="installation.html#docker-installation">Docker Installation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#environment-variables">Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#verification">Verification</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#serving-a-model">Serving a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#batch-inference">Batch Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#distributed-serving">Distributed Serving</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#api-server">API Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#performance-tips">Performance Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#next-steps">Next Steps</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture_guide.html">ATOM Architecture Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#system-overview">1. System Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#component-architecture">2. Component Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#request-lifecycle">3. Request Lifecycle</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#forward-context-pattern">4. Forward Context Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#multi-process-architecture">5. Multi-Process Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#sequence-lifecycle">6. Sequence Lifecycle</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="configuration_guide.html">ATOM Configuration Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#master-configuration-config">1. Master Configuration (<code class="docutils literal notranslate"><span class="pre">Config</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#compilation-configuration-compilationconfig">2. Compilation Configuration (<code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#compilation-levels-compilationlevel">2.1 Compilation Levels (<code class="docutils literal notranslate"><span class="pre">CompilationLevel</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#compilationconfig-fields">2.2 <code class="docutils literal notranslate"><span class="pre">CompilationConfig</span></code> Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#cuda-graph-mode-cudagraphmode">2.3 CUDA Graph Mode (<code class="docutils literal notranslate"><span class="pre">CUDAGraphMode</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#quantization-configuration-quantizationconfig">3. Quantization Configuration (<code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#quantizationconfig-fields">3.1 <code class="docutils literal notranslate"><span class="pre">QuantizationConfig</span></code> Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#quanttype-values-from-aiter">3.2 <code class="docutils literal notranslate"><span class="pre">QuantType</span></code> Values (from AITER)</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#supported-quantization-dtypes">3.3 Supported Quantization Dtypes</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#auto-detection-from-huggingface-get-quant-config">3.4 Auto-Detection from HuggingFace (<code class="docutils literal notranslate"><span class="pre">get_quant_config</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#parallel-configuration-parallelconfig">4. Parallel Configuration (<code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#speculative-decoding-configuration-speculativeconfig">5. Speculative Decoding Configuration (<code class="docutils literal notranslate"><span class="pre">SpeculativeConfig</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#sampling-parameters-samplingparams">6. Sampling Parameters (<code class="docutils literal notranslate"><span class="pre">SamplingParams</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#cli-arguments-engineargs">7. CLI Arguments (<code class="docutils literal notranslate"><span class="pre">EngineArgs</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#environment-variables">8. Environment Variables</a><ul>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#variables-registered-in-atom-utils-envs-py">8.1 Variables Registered in <code class="docutils literal notranslate"><span class="pre">atom/utils/envs.py</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration_guide.html#additional-environment-variables-used-outside-envs-py">8.2 Additional Environment Variables (Used Outside <code class="docutils literal notranslate"><span class="pre">envs.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#decision-tree-choosing-a-compilation-level">9. Decision Tree – Choosing a Compilation Level</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">ATOM Model Support Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-model-architectures">1. Supported Model Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-architecture-details">2. Model Architecture Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#qwen3-qwen3forcausallm">Qwen3 (<code class="docutils literal notranslate"><span class="pre">Qwen3ForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qwen3-moe-qwen3moeforcausallm">Qwen3-MoE (<code class="docutils literal notranslate"><span class="pre">Qwen3MoeForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#llama-llamaforcausallm">Llama (<code class="docutils literal notranslate"><span class="pre">LlamaForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mixtral-mixtralforcausallm">Mixtral (<code class="docutils literal notranslate"><span class="pre">MixtralForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deepseek-v2-v3-deepseekv2forcausallm">DeepSeek V2/V3 (<code class="docutils literal notranslate"><span class="pre">DeepseekV2ForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deepseek-mtp-deepseekmtp">DeepSeek MTP (<code class="docutils literal notranslate"><span class="pre">DeepSeekMTP</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpt-oss-gptossforcausallm">GPT-OSS (<code class="docutils literal notranslate"><span class="pre">GptOssForCausalLM</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#glm4-moe-glm4moeforcausallm">GLM4-MoE (<code class="docutils literal notranslate"><span class="pre">Glm4MoeForCausalLM</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#weight-loading">3. Weight Loading</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#function-signature">Function Signature</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-flow">Loading Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="#layers-beyond-num-hidden-layers">Layers Beyond <code class="docutils literal notranslate"><span class="pre">num_hidden_layers</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#adding-a-new-model">4. Adding a New Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-create-the-model-file">Step 1: Create the Model File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-implement-layer-classes">Step 2: Implement Layer Classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-implement-the-model-and-causallm-classes">Step 3: Implement the Model and CausalLM Classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-register-the-model">Step 4: Register the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-5-handle-weight-loading">Step 5: Handle Weight Loading</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-specific-optimizations">5. Model-Specific Optimizations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#llama-fused-rmsnorm-quant-and-silu-mul-quant">Llama: Fused RMSNorm+Quant and SiLU+Mul+Quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deepseek-v2-v3-mla-fused-input-norm-qk-norm-fusion">DeepSeek V2/V3: MLA + Fused Input Norm + QK Norm Fusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qwen3-moe-qk-norm-rope-cache-quant-fusion">Qwen3-MoE: QK Norm + RoPE + Cache + Quant Fusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mtp-deepseek-multi-token-prediction">MTP: DeepSeek Multi-Token Prediction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_ops_guide.html">ATOM Model Operations Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#aiter-integration-overview">1. AITER Integration Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#aiter-kernel-mapping-table">AITER Kernel Mapping Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#linear-operations">2. Linear Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#class-hierarchy">2.1 Class Hierarchy</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#quantization-dispatch">2.2 Quantization Dispatch</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#tensor-parallel-sharding">2.3 Tensor Parallel Sharding</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#weight-processing">2.4 Weight Processing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#attention-operations">3. Attention Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#base-attention-base-attention-py">3.1 Base: <code class="docutils literal notranslate"><span class="pre">Attention</span></code> (<code class="docutils literal notranslate"><span class="pre">base_attention.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#multi-head-attention-attention-mha-py">3.2 Multi-Head Attention (<code class="docutils literal notranslate"><span class="pre">attention_mha.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#multi-head-latent-attention-attention-mla-py">3.3 Multi-head Latent Attention (<code class="docutils literal notranslate"><span class="pre">attention_mla.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#backend-abstraction-attentions-backends-py">3.4 Backend Abstraction (<code class="docutils literal notranslate"><span class="pre">attentions/backends.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#kv-cache-operations">3.5 KV Cache Operations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#mixture-of-experts-moe">4. Mixture of Experts (MoE)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#fusedmoe-class-moe-py">4.1 <code class="docutils literal notranslate"><span class="pre">FusedMoE</span></code> Class (<code class="docutils literal notranslate"><span class="pre">moe.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#quantization-methods">4.2 Quantization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#topk-routing-topk-py">4.3 TopK Routing (<code class="docutils literal notranslate"><span class="pre">topK.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#fusedmoeparallelconfig">4.4 <code class="docutils literal notranslate"><span class="pre">FusedMoEParallelConfig</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#mori-integration-fused-moe-mori-prepare-finalize-py">4.5 MORI Integration (<code class="docutils literal notranslate"><span class="pre">fused_moe/mori_prepare_finalize.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#moe-quantization-config-fused-moe-config-py">4.6 MoE Quantization Config (<code class="docutils literal notranslate"><span class="pre">fused_moe/config.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#triton-moe-fallback-fused-moe-triton-py">4.7 Triton MoE Fallback (<code class="docutils literal notranslate"><span class="pre">fused_moe_triton.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#normalization">5. Normalization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#rmsnorm-layernorm-py">5.1 <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code> (<code class="docutils literal notranslate"><span class="pre">layernorm.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#layernorm-layernorm-py">5.2 <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> (<code class="docutils literal notranslate"><span class="pre">layernorm.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#activation-functions">6. Activation Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#siluandmul-activation-py">6.1 <code class="docutils literal notranslate"><span class="pre">SiluAndMul</span></code> (<code class="docutils literal notranslate"><span class="pre">activation.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#embedding-output-head">7. Embedding &amp; Output Head</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#vocabparallelembedding-embed-head-py">7.1 <code class="docutils literal notranslate"><span class="pre">VocabParallelEmbedding</span></code> (<code class="docutils literal notranslate"><span class="pre">embed_head.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#parallellmhead-embed-head-py">7.2 <code class="docutils literal notranslate"><span class="pre">ParallelLMHead</span></code> (<code class="docutils literal notranslate"><span class="pre">embed_head.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#rotary-position-embedding-rope">8. Rotary Position Embedding (RoPE)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#rotaryembedding-rotary-embedding-py">8.1 <code class="docutils literal notranslate"><span class="pre">RotaryEmbedding</span></code> (<code class="docutils literal notranslate"><span class="pre">rotary_embedding.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#get-rope-factory">8.2 <code class="docutils literal notranslate"><span class="pre">get_rope()</span></code> Factory</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#integration-in-attention">8.3 Integration in Attention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#sampling">9. Sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#sampler-sampler-py">9.1 <code class="docutils literal notranslate"><span class="pre">Sampler</span></code> (<code class="docutils literal notranslate"><span class="pre">sampler.py</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#rejectionsampler-rejection-sampler-py">9.2 <code class="docutils literal notranslate"><span class="pre">RejectionSampler</span></code> (<code class="docutils literal notranslate"><span class="pre">rejection_sampler.py</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#fused-kernel-chains">10. Fused Kernel Chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_ops_guide.html#source-files">Source Files</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-model-ops"><code class="docutils literal notranslate"><span class="pre">atom/model_ops/</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-model-ops-attentions"><code class="docutils literal notranslate"><span class="pre">atom/model_ops/attentions/</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-model-ops-fused-moe"><code class="docutils literal notranslate"><span class="pre">atom/model_ops/fused_moe/</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model_ops_guide.html#atom-utils"><code class="docutils literal notranslate"><span class="pre">atom/utils/</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="scheduling_kv_cache_guide.html">ATOM Scheduling &amp; KV Cache Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduling-algorithm">1. Scheduling Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduler-initialization">1.1 Scheduler Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#schedule-flow">1.2 Schedule Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#delay-factor">1.3 Delay Factor</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#preemption">1.4 Preemption</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduledbatch-structure">2. ScheduledBatch Structure</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#constructor-signature">2.1 Constructor Signature</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#fields">2.2 Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduledbatchoutput">2.3 ScheduledBatchOutput</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#block-manager">3. Block Manager</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#block-class">3.1 Block Class</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#blockmanager-initialization">3.2 BlockManager Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#allocation-allocate">3.3 Allocation (<code class="docutils literal notranslate"><span class="pre">allocate</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#deallocation-deallocate">3.4 Deallocation (<code class="docutils literal notranslate"><span class="pre">deallocate</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#can-allocate-and-can-append-checks">3.5 Can-Allocate and Can-Append Checks</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#may-append-decode-extension">3.6 May-Append (Decode Extension)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#prefix-caching">4. Prefix Caching</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#hash-function">4.1 Hash Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#hash-chaining">4.2 Hash Chaining</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#cache-lookup-during-allocation">4.3 Cache Lookup During Allocation</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#reference-counting">4.4 Reference Counting</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#enabling-prefix-caching">4.5 Enabling Prefix Caching</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#postprocessing">5. Postprocessing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#signature">5.1 Signature</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#token-appending">5.2 Token Appending</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#stop-condition-checking">5.3 Stop Condition Checking</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#stream-output">5.4 Stream Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequence-cleanup">5.5 Sequence Cleanup</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#placeholder-insertion">5.6 Placeholder Insertion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#speculative-decoding-integration">6. Speculative Decoding Integration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#scheduler-tracking">6.1 Scheduler Tracking</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#draft-tokens-in-scheduling">6.2 Draft Tokens in Scheduling</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#acceptance-statistics">6.3 Acceptance Statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#draft-token-storage-on-sequences">6.4 Draft Token Storage on Sequences</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequence-management">7. Sequence Management</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#constructor">7.1 Constructor</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#core-fields">7.2 Core Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#timing-fields">7.3 Timing Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#computed-properties">7.4 Computed Properties</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#num-tokens-setter">7.5 num_tokens Setter</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#lifecycle">7.6 Lifecycle</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequencestatus-enum">7.7 SequenceStatus Enum</a></li>
<li class="toctree-l3"><a class="reference internal" href="scheduling_kv_cache_guide.html#sequencetype-enum">7.8 SequenceType Enum</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scheduling_kv_cache_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_guide.html">ATOM Distributed Inference Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#tensor-parallelism-tp">1. Tensor Parallelism (TP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#weight-sharding">Weight Sharding</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#process-group-initialization">Process Group Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#allreduce">AllReduce</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#configuration">Configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#data-parallelism-dp">2. Data Parallelism (DP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#architecture">Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#dp-process-group-initialization">DP Process Group Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#synchronized-busy-loop">Synchronized Busy Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#dummy-batch-execution">Dummy Batch Execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#device-assignment">Device Assignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#dpmetadata">DPMetadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#coremanager-dp-orchestration">CoreManager (DP Orchestration)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#id1">Configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#expert-parallelism-ep">3. Expert Parallelism (EP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#fusedmoeparallelconfig">FusedMoEParallelConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#expert-distribution">Expert Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#mori-communication">MORI Communication</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#id2">Configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#environment-variables">4. Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#multi-gpu-deployment-examples">5. Multi-GPU Deployment Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#deepseek-r1-on-8-gpus-tp8">DeepSeek-R1 on 8 GPUs (TP8)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#qwen3-235b-a22b-on-8-gpus-tp8-ep">Qwen3-235B-A22B on 8 GPUs (TP8 + EP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#kimi-k2-thinking-on-4-gpus-tp4">Kimi-K2-Thinking on 4 GPUs (TP4)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#combined-parallelism-strategies">6. Combined Parallelism Strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#tp-only-dense-models">TP Only (Dense Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#tp-ep-moe-models">TP + EP (MoE Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#tp-dp-dense-throughput">TP + DP (Dense Throughput)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#tp-dp-ep-moe-throughput">TP + DP + EP (MoE Throughput)</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed_guide.html#dp-attention-mode">DP Attention Mode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="compilation_cudagraph_guide.html">ATOM Compilation &amp; CUDA Graphs Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#compilation-levels">1. Compilation Levels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-0-no-compilation">Level 0 – NO_COMPILATION</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-1-dynamo-as-is">Level 1 – DYNAMO_AS_IS</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-2-dynamo-once">Level 2 – DYNAMO_ONCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#level-3-piecewise-production-default">Level 3 – PIECEWISE (Production Default)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#cuda-graph-modes">2. CUDA Graph Modes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#none-value-0">NONE (value: 0)</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#piecewise-value-1">PIECEWISE (value: 1)</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#full-value-2">FULL (value: 2)</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#full-decode-only-value-full-none">FULL_DECODE_ONLY (value: (FULL, NONE))</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#full-and-piecewise-value-full-piecewise">FULL_AND_PIECEWISE (value: (FULL, PIECEWISE))</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#helper-methods">Helper Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#cuda-graph-capture">3. CUDA Graph Capture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#capture-flow">Capture Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#graph-keying">Graph Keying</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#graph-pool-sharing">Graph Pool Sharing</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#default-capture-sizes">Default Capture Sizes</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#graph-replay-in-run-model">Graph Replay in run_model()</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#piecewise-compilation">4. Piecewise Compilation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#splitting-operations">Splitting Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#compilation-pipeline">Compilation Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#cache-management">Cache Management</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#forward-context-stateless-dispatch">5. Forward Context &amp; Stateless Dispatch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#forwardcontext-fields">ForwardContext Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#lifecycle">Lifecycle</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#context-dataclass">Context Dataclass</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#integration-with-cuda-graphs">Integration with CUDA Graphs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#compiler-backend">6. Compiler Backend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#compilermanager">CompilerManager</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#compilerinterface">CompilerInterface</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#inductoradaptor">InductorAdaptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#inductorstandaloneadaptor">InductorStandaloneAdaptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#vllmbackend">VllmBackend</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#support-torch-compile-decorator">&#64;support_torch_compile Decorator</a></li>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#custom-op-registration">Custom Op Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#configuration-options">7. Configuration Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#decision-tree">8. Decision Tree</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compilation_cudagraph_guide.html#common-configurations">Common Configurations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compilation_cudagraph_guide.html#source-files">Source Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="serving_benchmarking_guide.html">ATOM Serving &amp; Benchmarking Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#openai-compatible-server">1. OpenAI-Compatible Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#endpoints">1.1 Endpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#request-models">1.2 Request Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#response-models">1.3 Response Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#server-startup">1.4 Server Startup</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#example-curl">1.5 Example: curl</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#programmatic-api-llmengine">2. Programmatic API (LLMEngine)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#initialization">2.1 Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#samplingparams">2.2 SamplingParams</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#core-methods">2.3 Core Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#synchronous-generation-example">2.4 Synchronous Generation Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#asynchronous-streaming-usage">2.5 Asynchronous / Streaming Usage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#simple-inference">3. Simple Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#usage">3.1 Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#what-it-does">3.2 What It Does</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#benchmarking">4. Benchmarking</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#metrics">4.1 Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#key-cli-arguments">4.2 Key CLI Arguments</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#backend-request-functions">4.3 Backend Request Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#full-benchmark-example">4.4 Full Benchmark Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#profiling">5. Profiling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#configuration">5.1 Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#online-profiling-http">5.2 Online Profiling (HTTP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#programmatic-profiling">5.3 Programmatic Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#offline-profiling-script">5.4 Offline Profiling Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#profiling-during-benchmarks">5.5 Profiling During Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#speculative-decoding-mtp">6. Speculative Decoding (MTP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#architecture">6.1 Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#id1">6.2 Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#mtp-statistics">6.3 MTP Statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#how-rejection-sampling-works">6.4 How Rejection Sampling Works</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#deployment-examples">7. Deployment Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#single-gpu">7.1 Single-GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#multi-gpu-with-tensor-parallelism">7.2 Multi-GPU with Tensor Parallelism</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#docker-deployment">7.3 Docker Deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#engine-cli-arguments-engineargs">7.4 Engine CLI Arguments (EngineArgs)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#accuracy-validation">8. Accuracy Validation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#setup">8.1 Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="serving_benchmarking_guide.html#run-evaluation">8.2 Run Evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="serving_benchmarking_guide.html#source-files">Source Files</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api/serving.html">Serving API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#llm-class">LLM Class</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/serving.html#methods">Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/serving.html#generate">generate()</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#samplingparams">SamplingParams</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#requestoutput">RequestOutput</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/serving.html#example">Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/models.html">Supported Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#llama-models">Llama Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#gpt-models">GPT Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#mixtral">Mixtral</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#other-architectures">Other Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#performance-by-model-size">Performance by Model Size</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#quantization">Quantization</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #C00000" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ATOM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">ATOM Model Support Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/model_support_guide.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="atom-model-support-guide">
<h1>ATOM Model Support Guide<a class="headerlink" href="#atom-model-support-guide" title="Link to this heading"></a></h1>
<p>ATOM (AiTer Optimized Model) is AMD’s lightweight LLM inference engine built on AITER kernels for ROCm/HIP GPUs. This guide covers the supported model architectures, weight loading, and how to add new models.</p>
<section id="quick-reference">
<h2>Quick Reference<a class="headerlink" href="#quick-reference" title="Link to this heading"></a></h2>
<p>The model registry lives in <code class="docutils literal notranslate"><span class="pre">atom/model_engine/model_runner.py</span></code> as <code class="docutils literal notranslate"><span class="pre">support_model_arch_dict</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">support_model_arch_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Qwen3ForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;atom.models.qwen3.Qwen3ForCausalLM&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Qwen3MoeForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;atom.models.qwen3_moe.Qwen3MoeForCausalLM&quot;</span><span class="p">,</span>
    <span class="s2">&quot;LlamaForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;atom.models.llama.LlamaForCausalLM&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MixtralForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;atom.models.mixtral.MixtralForCausalLM&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DeepseekV3ForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;atom.models.deepseek_v2.DeepseekV2ForCausalLM&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DeepseekV32ForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;atom.models.deepseek_v2.DeepseekV2ForCausalLM&quot;</span><span class="p">,</span>
    <span class="s2">&quot;GptOssForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;atom.models.gpt_oss.GptOssForCausalLM&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Glm4MoeForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;atom.models.glm4_moe.Glm4MoeForCausalLM&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>ATOM resolves the HuggingFace <code class="docutils literal notranslate"><span class="pre">architectures</span></code> field from a model’s <code class="docutils literal notranslate"><span class="pre">config.json</span></code> against this dictionary. If the architecture string matches a key, ATOM imports and instantiates the corresponding class.</p>
</section>
<hr class="docutils" />
<section id="supported-model-architectures">
<h2>1. Supported Model Architectures<a class="headerlink" href="#supported-model-architectures" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>HF Architecture</p></th>
<th class="head"><p>ATOM Module</p></th>
<th class="head"><p>ATOM Class</p></th>
<th class="head"><p>MoE</p></th>
<th class="head"><p>MLA</p></th>
<th class="head"><p>Key Features</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Qwen3ForCausalLM</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom.models.qwen3</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Qwen3ForCausalLM</span></code></p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>GQA, QK norm, RoPE</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">Qwen3MoeForCausalLM</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom.models.qwen3_moe</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Qwen3MoeForCausalLM</span></code></p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>GQA, QK norm, FusedMoE, sparse+dense layer mixing, QK norm+RoPE+cache+quant fusion</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">LlamaForCausalLM</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom.models.llama</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LlamaForCausalLM</span></code></p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>GQA, RoPE, fused RMSNorm+quant, fused SiLU+mul+quant</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">MixtralForCausalLM</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom.models.mixtral</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MixtralForCausalLM</span></code></p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>GQA, RoPE, FusedMoE with TP sharding</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">DeepseekV3ForCausalLM</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom.models.deepseek_v2</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">DeepseekV2ForCausalLM</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>MLA attention, LoRA-compressed QKV, FusedMoE with shared experts, FP4/FP8 fused kernels</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">DeepseekV32ForCausalLM</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom.models.deepseek_v2</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">DeepseekV2ForCausalLM</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Same as above with V3.2 index-based top-k routing</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GptOssForCausalLM</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom.models.gpt_oss</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GptOssForCausalLM</span></code></p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>GQA, RoPE, sliding window attention (every other layer), attention sinks, bias in QKV and MoE</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">Glm4MoeForCausalLM</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">atom.models.glm4_moe</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Glm4MoeForCausalLM</span></code></p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>GQA, partial RoPE (0.5 factor), QK norm, shared+routed experts, sigmoid scoring, grouped top-k</p></td>
</tr>
</tbody>
</table>
<p><strong>Note:</strong> <code class="docutils literal notranslate"><span class="pre">DeepSeekMTP</span></code> (<code class="docutils literal notranslate"><span class="pre">atom.models.deepseek_mtp.DeepSeekMTP</span></code>) is not in the registry – it is used exclusively as a speculative draft model for DeepSeek multi-token prediction and is loaded separately.</p>
</section>
<hr class="docutils" />
<section id="model-architecture-details">
<h2>2. Model Architecture Details<a class="headerlink" href="#model-architecture-details" title="Link to this heading"></a></h2>
<section id="qwen3-qwen3forcausallm">
<h3>Qwen3 (<code class="docutils literal notranslate"><span class="pre">Qwen3ForCausalLM</span></code>)<a class="headerlink" href="#qwen3-qwen3forcausallm" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Architecture:</strong> Dense transformer with Grouped-Query Attention (GQA).</p></li>
<li><p><strong>Layer structure:</strong> <code class="docutils literal notranslate"><span class="pre">Qwen3DecoderLayer</span></code> containing <code class="docutils literal notranslate"><span class="pre">Qwen3Attention</span></code> + <code class="docutils literal notranslate"><span class="pre">Qwen3MLP</span></code>.</p></li>
<li><p><strong>Attention:</strong> <code class="docutils literal notranslate"><span class="pre">QKVParallelLinear</span></code> for fused QKV projection, per-head QK RMSNorm (<code class="docutils literal notranslate"><span class="pre">q_norm</span></code>, <code class="docutils literal notranslate"><span class="pre">k_norm</span></code>), RoPE, <code class="docutils literal notranslate"><span class="pre">RowParallelLinear</span></code> for output projection.</p></li>
<li><p><strong>MLP:</strong> <code class="docutils literal notranslate"><span class="pre">MergedColumnParallelLinear</span></code> for gate+up projection, SiLU activation, <code class="docutils literal notranslate"><span class="pre">RowParallelLinear</span></code> for down projection.</p></li>
<li><p><strong>Normalization:</strong> RMSNorm on input and post-attention.</p></li>
</ul>
</section>
<section id="qwen3-moe-qwen3moeforcausallm">
<h3>Qwen3-MoE (<code class="docutils literal notranslate"><span class="pre">Qwen3MoeForCausalLM</span></code>)<a class="headerlink" href="#qwen3-moe-qwen3moeforcausallm" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Architecture:</strong> Mixture-of-Experts transformer with GQA.</p></li>
<li><p><strong>Layer structure:</strong> <code class="docutils literal notranslate"><span class="pre">Qwen3MoeDecoderLayer</span></code> containing <code class="docutils literal notranslate"><span class="pre">Qwen3MoeAttention</span></code> + either <code class="docutils literal notranslate"><span class="pre">Qwen3MoeSparseMoeBlock</span></code> (MoE layers) or <code class="docutils literal notranslate"><span class="pre">Qwen3MoeMLP</span></code> (dense layers, controlled by <code class="docutils literal notranslate"><span class="pre">mlp_only_layers</span></code> and <code class="docutils literal notranslate"><span class="pre">decoder_sparse_step</span></code>).</p></li>
<li><p><strong>Attention:</strong> Same QKV structure as Qwen3 with QK norm. Supports QK norm + RoPE + cache + quant fusion when <code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_QK_NORM_ROPE_CACHE_QUANT_FUSION</span></code> is set – this precomputes a joint <code class="docutils literal notranslate"><span class="pre">cos_sin_cache</span></code> and passes <code class="docutils literal notranslate"><span class="pre">q_norm</span></code>/<code class="docutils literal notranslate"><span class="pre">k_norm</span></code> to the <code class="docutils literal notranslate"><span class="pre">Attention</span></code> module.</p></li>
<li><p><strong>MoE:</strong> <code class="docutils literal notranslate"><span class="pre">FusedMoE</span></code> with <code class="docutils literal notranslate"><span class="pre">ReplicatedLinear</span></code> gate router. Supports allreduce+RMSNorm fusion (<code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_ALLREDUCE_RMSNORM_FUSION</span></code>).</p></li>
<li><p><strong>Normalization:</strong> RMSNorm with optional fused allreduce.</p></li>
</ul>
</section>
<section id="llama-llamaforcausallm">
<h3>Llama (<code class="docutils literal notranslate"><span class="pre">LlamaForCausalLM</span></code>)<a class="headerlink" href="#llama-llamaforcausallm" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Architecture:</strong> Dense transformer with GQA. Covers Llama 2/3 and compatible architectures (InternLM, Mistral-Nemo via optional <code class="docutils literal notranslate"><span class="pre">head_dim</span></code>).</p></li>
<li><p><strong>Layer structure:</strong> <code class="docutils literal notranslate"><span class="pre">LlamaDecoderLayer</span></code> containing <code class="docutils literal notranslate"><span class="pre">LlamaAttention</span></code> + <code class="docutils literal notranslate"><span class="pre">LlamaMLP</span></code>.</p></li>
<li><p><strong>Attention:</strong> <code class="docutils literal notranslate"><span class="pre">QKVParallelLinear</span></code>, RoPE (NeoX or original style based on GGUF), per-layer sliding window support via <code class="docutils literal notranslate"><span class="pre">layer_types</span></code> config.</p></li>
<li><p><strong>MLP:</strong> <code class="docutils literal notranslate"><span class="pre">MergedColumnParallelLinear</span></code> for gate+up, SiLU+mul activation, <code class="docutils literal notranslate"><span class="pre">RowParallelLinear</span></code> for down.</p></li>
<li><p><strong>Fused optimizations:</strong> Controlled by environment variables:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ATOM_LLAMA_ENABLE_AITER_TRITON_FUSED_RMSNORM_QUANT</span></code> – fuses RMSNorm with FP8/MXFP4 quantization.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ATOM_LLAMA_ENABLE_AITER_TRITON_FUSED_SILU_MUL_QUANT</span></code> – fuses SiLU+mul activation with quantization.</p></li>
</ul>
</li>
<li><p><strong>Pipeline parallelism:</strong> Full PP support with <code class="docutils literal notranslate"><span class="pre">PPMissingLayer</span></code> placeholders and <code class="docutils literal notranslate"><span class="pre">IntermediateTensors</span></code> for cross-stage communication. Supports auxiliary hidden state extraction for speculative decoding.</p></li>
</ul>
</section>
<section id="mixtral-mixtralforcausallm">
<h3>Mixtral (<code class="docutils literal notranslate"><span class="pre">MixtralForCausalLM</span></code>)<a class="headerlink" href="#mixtral-mixtralforcausallm" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Architecture:</strong> Sparse Mixture-of-Experts with GQA.</p></li>
<li><p><strong>Layer structure:</strong> <code class="docutils literal notranslate"><span class="pre">MixtralDecoderLayer</span></code> containing <code class="docutils literal notranslate"><span class="pre">MixtralAttention</span></code> + <code class="docutils literal notranslate"><span class="pre">MixtralMoE</span></code>.</p></li>
<li><p><strong>Attention:</strong> Standard GQA with <code class="docutils literal notranslate"><span class="pre">QKVParallelLinear</span></code>, RoPE (NeoX style), <code class="docutils literal notranslate"><span class="pre">RowParallelLinear</span></code>.</p></li>
<li><p><strong>MoE:</strong> <code class="docutils literal notranslate"><span class="pre">MixtralMoE</span></code> wraps <code class="docutils literal notranslate"><span class="pre">ReplicatedLinear</span></code> gate + <code class="docutils literal notranslate"><span class="pre">FusedMoE</span></code>. Experts are sharded across TP ranks with full reduce. Gate checkpoint names use <code class="docutils literal notranslate"><span class="pre">w1</span></code>/<code class="docutils literal notranslate"><span class="pre">w2</span></code>/<code class="docutils literal notranslate"><span class="pre">w3</span></code> convention (mapped to <code class="docutils literal notranslate"><span class="pre">gate_proj</span></code>/<code class="docutils literal notranslate"><span class="pre">down_proj</span></code>/<code class="docutils literal notranslate"><span class="pre">up_proj</span></code>).</p></li>
<li><p><strong>Normalization:</strong> RMSNorm.</p></li>
</ul>
</section>
<section id="deepseek-v2-v3-deepseekv2forcausallm">
<h3>DeepSeek V2/V3 (<code class="docutils literal notranslate"><span class="pre">DeepseekV2ForCausalLM</span></code>)<a class="headerlink" href="#deepseek-v2-v3-deepseekv2forcausallm" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Architecture:</strong> MoE transformer with Multi-head Latent Attention (MLA).</p></li>
<li><p><strong>Layer structure:</strong> <code class="docutils literal notranslate"><span class="pre">DeepseekV2DecoderLayer</span></code> containing <code class="docutils literal notranslate"><span class="pre">DeepseekV2MLAAttention</span></code> + either <code class="docutils literal notranslate"><span class="pre">DeepseekV2MoE</span></code> (MoE layers) or <code class="docutils literal notranslate"><span class="pre">DeepseekV2MLP</span></code> (dense layers).</p></li>
<li><p><strong>MLA Attention:</strong> Uses LoRA-compressed QKV (<code class="docutils literal notranslate"><span class="pre">q_lora_rank</span></code>, <code class="docutils literal notranslate"><span class="pre">kv_lora_rank</span></code>), separate <code class="docutils literal notranslate"><span class="pre">qk_nope_head_dim</span></code> and <code class="docutils literal notranslate"><span class="pre">qk_rope_head_dim</span></code> for non-positional and rotary-embedded components. Backed by <code class="docutils literal notranslate"><span class="pre">MLAModules</span></code> from <code class="docutils literal notranslate"><span class="pre">atom.model_ops.attention_mla</span></code>.</p></li>
<li><p><strong>MoE:</strong> <code class="docutils literal notranslate"><span class="pre">DeepseekV2MoE</span></code> with routed + shared experts. Supports shared expert fusion (<code class="docutils literal notranslate"><span class="pre">is_rocm_aiter_fusion_shared_expert_enabled</span></code>), routed scaling factor fusion (<code class="docutils literal notranslate"><span class="pre">is_rocm_aiter_fuse_routed_scaling_factor</span></code>), and grouped top-k routing.</p></li>
<li><p><strong>Fused optimizations:</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_DS_INPUT_RMSNORM_QUANT_FUSION</span></code> – fuses input RMSNorm with FP8/FP4 quantization.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_DS_QKNORM_QUANT_FUSION</span></code> – fuses QK norm with quantization.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_ALLREDUCE_RMSNORM_FUSION</span></code> – fuses allreduce with RMSNorm.</p></li>
<li><p>Dedicated Triton kernels for FP8 MQA logits (<code class="docutils literal notranslate"><span class="pre">fp8_mqa_logits</span></code>), paged MQA logits (<code class="docutils literal notranslate"><span class="pre">deepgemm_fp8_paged_mqa_logits</span></code>), and fused RMSNorm+quantization (<code class="docutils literal notranslate"><span class="pre">_fuse_rmsnorm_quant</span></code>).</p></li>
</ul>
</li>
<li><p><strong>V3.2 extension:</strong> <code class="docutils literal notranslate"><span class="pre">DeepseekV32ForCausalLM</span></code> is an alias. The <code class="docutils literal notranslate"><span class="pre">DeepseekV2Model</span></code> detects V3.2 via <code class="docutils literal notranslate"><span class="pre">config.index_topk</span></code> and allocates an <code class="docutils literal notranslate"><span class="pre">topk_indices_buffer</span></code> for index-based routing.</p></li>
<li><p><strong>Note:</strong> <code class="docutils literal notranslate"><span class="pre">DeepseekV3ForCausalLM</span></code> is a subclass of <code class="docutils literal notranslate"><span class="pre">DeepseekV2ForCausalLM</span></code> (pass-through, no override).</p></li>
</ul>
</section>
<section id="deepseek-mtp-deepseekmtp">
<h3>DeepSeek MTP (<code class="docutils literal notranslate"><span class="pre">DeepSeekMTP</span></code>)<a class="headerlink" href="#deepseek-mtp-deepseekmtp" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Architecture:</strong> Multi-Token Prediction draft model for speculative decoding.</p></li>
<li><p><strong>Layer structure:</strong> <code class="docutils literal notranslate"><span class="pre">DeepSeekMultiTokenPredictor</span></code> containing one or more <code class="docutils literal notranslate"><span class="pre">DeepSeekMultiTokenPredictorLayer</span></code>, each with <code class="docutils literal notranslate"><span class="pre">enorm</span></code> (embedding norm), <code class="docutils literal notranslate"><span class="pre">hnorm</span></code> (hidden state norm), <code class="docutils literal notranslate"><span class="pre">eh_proj</span></code> (linear projection joining embedded+hidden), <code class="docutils literal notranslate"><span class="pre">mtp_block</span></code> (a <code class="docutils literal notranslate"><span class="pre">DeepseekV2DecoderLayer</span></code>), and a <code class="docutils literal notranslate"><span class="pre">SharedHead</span></code> (norm + LM head).</p></li>
<li><p><strong>Usage:</strong> Not registered in <code class="docutils literal notranslate"><span class="pre">support_model_arch_dict</span></code>. Loaded separately with <code class="docutils literal notranslate"><span class="pre">spec_decode=True</span></code> in <code class="docutils literal notranslate"><span class="pre">load_model()</span></code>, which invokes <code class="docutils literal notranslate"><span class="pre">rewrite_spec_layer_name()</span></code> to remap MTP weight names (e.g., adding <code class="docutils literal notranslate"><span class="pre">.mtp_block.</span></code> prefix for transformer layer weights, remapping <code class="docutils literal notranslate"><span class="pre">embed_tokens</span></code> to top-level).</p></li>
<li><p><strong>MTP layers start</strong> at <code class="docutils literal notranslate"><span class="pre">config.num_hidden_layers</span></code> (i.e., the layer indices following the main model layers).</p></li>
</ul>
</section>
<section id="gpt-oss-gptossforcausallm">
<h3>GPT-OSS (<code class="docutils literal notranslate"><span class="pre">GptOssForCausalLM</span></code>)<a class="headerlink" href="#gpt-oss-gptossforcausallm" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Architecture:</strong> MoE transformer with GQA and alternating sliding window attention.</p></li>
<li><p><strong>Layer structure:</strong> <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code> containing <code class="docutils literal notranslate"><span class="pre">OAIAttention</span></code> + <code class="docutils literal notranslate"><span class="pre">MLPBlock</span></code>.</p></li>
<li><p><strong>Attention:</strong> <code class="docutils literal notranslate"><span class="pre">OAIAttention</span></code> with bias on QKV and output projections, attention sinks (learnable per-head parameters), and sliding window applied on even-indexed layers only.</p></li>
<li><p><strong>MoE:</strong> <code class="docutils literal notranslate"><span class="pre">MLPBlock</span></code> wraps <code class="docutils literal notranslate"><span class="pre">ReplicatedLinear</span></code> router (with bias) + <code class="docutils literal notranslate"><span class="pre">FusedMoE</span></code> with SwiGLU activation and bias support. Custom <code class="docutils literal notranslate"><span class="pre">weights_mapping</span></code> translates checkpoint names (<code class="docutils literal notranslate"><span class="pre">gate_up_proj_blocks</span></code> to <code class="docutils literal notranslate"><span class="pre">w13_weight</span></code>, etc.).</p></li>
<li><p><strong>Normalization:</strong> RMSNorm with eps=1e-5, post-attention norm uses <code class="docutils literal notranslate"><span class="pre">x_pad_to_multiple=256</span></code>.</p></li>
<li><p><strong>Pipeline parallelism:</strong> Supports auxiliary hidden state layers for EAGLE3 speculative decoding (<code class="docutils literal notranslate"><span class="pre">get_eagle3_aux_hidden_state_layers</span></code>).</p></li>
</ul>
</section>
<section id="glm4-moe-glm4moeforcausallm">
<h3>GLM4-MoE (<code class="docutils literal notranslate"><span class="pre">Glm4MoeForCausalLM</span></code>)<a class="headerlink" href="#glm4-moe-glm4moeforcausallm" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Architecture:</strong> MoE transformer with GQA, shared + routed experts, partial RoPE.</p></li>
<li><p><strong>Layer structure:</strong> <code class="docutils literal notranslate"><span class="pre">Glm4MoeDecoderLayer</span></code> containing <code class="docutils literal notranslate"><span class="pre">Glm4MoeAttention</span></code> + either <code class="docutils literal notranslate"><span class="pre">Glm4MoE</span></code> (MoE layers, from <code class="docutils literal notranslate"><span class="pre">first_k_dense_replace</span></code> onward) or <code class="docutils literal notranslate"><span class="pre">Glm4MoeMLP</span></code> (dense layers).</p></li>
<li><p><strong>Attention:</strong> <code class="docutils literal notranslate"><span class="pre">Glm4MoeAttention</span></code> with optional QK norm (<code class="docutils literal notranslate"><span class="pre">use_qk_norm</span></code>), partial rotary factor of 0.5.</p></li>
<li><p><strong>MoE:</strong> <code class="docutils literal notranslate"><span class="pre">Glm4MoE</span></code> with sigmoid scoring, <code class="docutils literal notranslate"><span class="pre">e_score_correction_bias</span></code>, grouped top-k routing (<code class="docutils literal notranslate"><span class="pre">n_group</span></code>, <code class="docutils literal notranslate"><span class="pre">topk_group</span></code>), routed scaling factor. Shared experts handled separately or fused into <code class="docutils literal notranslate"><span class="pre">FusedMoE</span></code> via <code class="docutils literal notranslate"><span class="pre">is_rocm_aiter_fusion_shared_expert_enabled()</span></code>. Expert parallelism (EP) support built in.</p></li>
<li><p><strong>Inherits:</strong> <code class="docutils literal notranslate"><span class="pre">Glm4MixtureOfExperts</span></code> mixin for MoE metadata management and expert load balancing (EPLB) support.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="weight-loading">
<h2>3. Weight Loading<a class="headerlink" href="#weight-loading" title="Link to this heading"></a></h2>
<p>Weight loading is handled by <code class="docutils literal notranslate"><span class="pre">load_model()</span></code> in <code class="docutils literal notranslate"><span class="pre">atom/model_loader/loader.py</span></code>.</p>
<section id="function-signature">
<h3>Function Signature<a class="headerlink" href="#function-signature" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">load_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">hf_config</span><span class="p">:</span> <span class="n">AutoConfig</span><span class="p">,</span>
    <span class="n">load_dummy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">spec_decode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
</pre></div>
</div>
</section>
<section id="loading-flow">
<h3>Loading Flow<a class="headerlink" href="#loading-flow" title="Link to this heading"></a></h3>
<ol class="arabic">
<li><p><strong>SafeTensors iteration:</strong> <code class="docutils literal notranslate"><span class="pre">safetensors_weights_iterator()</span></code> discovers and iterates over all <code class="docutils literal notranslate"><span class="pre">*.safetensors</span></code> files in the model directory (or downloads them from HuggingFace Hub via <code class="docutils literal notranslate"><span class="pre">download_weights_from_hf()</span></code>). Duplicate files are filtered using the <code class="docutils literal notranslate"><span class="pre">model.safetensors.index.json</span></code> weight map. Memory-mapped loading is used by default; set <code class="docutils literal notranslate"><span class="pre">ATOM_DISABLE_MMAP=true</span></code> to disable.</p></li>
<li><p><strong>Weight name rewriting:</strong> Each weight name goes through several transformations:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weight_scale_inv</span></code> is renamed to <code class="docutils literal notranslate"><span class="pre">weight_scale</span></code>.</p></li>
<li><p>Model-specific <code class="docutils literal notranslate"><span class="pre">weights_mapping</span></code> (e.g., GPT-OSS maps <code class="docutils literal notranslate"><span class="pre">gate_up_proj_blocks</span></code> to <code class="docutils literal notranslate"><span class="pre">w13_weight</span></code>).</p></li>
<li><p>For speculative decoding (<code class="docutils literal notranslate"><span class="pre">spec_decode=True</span></code>), MTP layer weights are rewritten via <code class="docutils literal notranslate"><span class="pre">rewrite_spec_layer_name()</span></code>.</p></li>
<li><p>Shared expert fusion: when enabled, <code class="docutils literal notranslate"><span class="pre">mlp.shared_experts</span></code> is remapped to <code class="docutils literal notranslate"><span class="pre">mlp.experts.&lt;n_routed_experts&gt;</span></code> so the shared expert is loaded as the last expert in the <code class="docutils literal notranslate"><span class="pre">FusedMoE</span></code> module.</p></li>
</ul>
</li>
<li><p><strong>Packed module resolution:</strong> The <code class="docutils literal notranslate"><span class="pre">packed_modules_mapping</span></code> dict on each model class defines how HuggingFace checkpoint weight names map to ATOM’s fused parameter names. For example, Llama maps:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;q_proj&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;qkv_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;q&quot;</span><span class="p">),</span>
<span class="s2">&quot;k_proj&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;qkv_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;k&quot;</span><span class="p">),</span>
<span class="s2">&quot;v_proj&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;qkv_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">),</span>
<span class="s2">&quot;gate_proj&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;gate_up_proj&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
<span class="s2">&quot;up_proj&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;gate_up_proj&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</pre></div>
</div>
<p>Each packed parameter has a <code class="docutils literal notranslate"><span class="pre">weight_loader</span></code> attribute that knows how to shard and place the weight into the correct slice.</p>
</li>
<li><p><strong>Expert parameter loading:</strong> If the model has a <code class="docutils literal notranslate"><span class="pre">get_expert_mapping()</span></code> method, expert weights are loaded using <code class="docutils literal notranslate"><span class="pre">FusedMoE.make_expert_params_mapping()</span></code>, which generates (param_name, weight_name, expert_id, shard_id) tuples. This handles per-expert sharding across TP ranks.</p></li>
<li><p><strong>TP sharding:</strong> Parallel linear layers (<code class="docutils literal notranslate"><span class="pre">ColumnParallelLinear</span></code>, <code class="docutils literal notranslate"><span class="pre">RowParallelLinear</span></code>, <code class="docutils literal notranslate"><span class="pre">QKVParallelLinear</span></code>) have custom <code class="docutils literal notranslate"><span class="pre">weight_loader</span></code> methods that automatically select the correct shard for the current TP rank during loading. The default fallback <code class="docutils literal notranslate"><span class="pre">default_weight_loader</span></code> handles simple cases where weights need to be sliced by TP rank.</p></li>
<li><p><strong>Concurrent loading:</strong> All weight loading calls are submitted to a <code class="docutils literal notranslate"><span class="pre">ThreadPoolExecutor</span></code> for parallel execution.</p></li>
<li><p><strong>Post-processing:</strong> After all weights are loaded, <code class="docutils literal notranslate"><span class="pre">process_weights_after_loading()</span></code> is called on each module (e.g., for weight pre-shuffling, scale computation), and <code class="docutils literal notranslate"><span class="pre">quant_method.process_weights_after_loading()</span></code> is invoked for quantized modules. For <code class="docutils literal notranslate"><span class="pre">FusedMoEMethodBase</span></code>, <code class="docutils literal notranslate"><span class="pre">init_prepare_finalize()</span></code> is also called.</p></li>
</ol>
</section>
<section id="layers-beyond-num-hidden-layers">
<h3>Layers Beyond <code class="docutils literal notranslate"><span class="pre">num_hidden_layers</span></code><a class="headerlink" href="#layers-beyond-num-hidden-layers" title="Link to this heading"></a></h3>
<p>Weights for layers with index &gt;= <code class="docutils literal notranslate"><span class="pre">config.num_hidden_layers</span></code> are skipped during normal loading. These layers (MTP layers) are only loaded when <code class="docutils literal notranslate"><span class="pre">spec_decode=True</span></code>.</p>
</section>
</section>
<hr class="docutils" />
<section id="adding-a-new-model">
<h2>4. Adding a New Model<a class="headerlink" href="#adding-a-new-model" title="Link to this heading"></a></h2>
<p>Follow these steps to add support for a new model architecture:</p>
<section id="step-1-create-the-model-file">
<h3>Step 1: Create the Model File<a class="headerlink" href="#step-1-create-the-model-file" title="Link to this heading"></a></h3>
<p>Create a new file in <code class="docutils literal notranslate"><span class="pre">atom/models/</span></code>, e.g., <code class="docutils literal notranslate"><span class="pre">atom/models/my_model.py</span></code>. Follow the existing patterns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">atom.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">Config</span><span class="p">,</span> <span class="n">QuantizationConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">atom.model_ops.base_attention</span><span class="w"> </span><span class="kn">import</span> <span class="n">Attention</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">atom.model_ops.embed_head</span><span class="w"> </span><span class="kn">import</span> <span class="n">ParallelLMHead</span><span class="p">,</span> <span class="n">VocabParallelEmbedding</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">atom.model_ops.layernorm</span><span class="w"> </span><span class="kn">import</span> <span class="n">RMSNorm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">atom.model_ops.linear</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">MergedColumnParallelLinear</span><span class="p">,</span>
    <span class="n">QKVParallelLinear</span><span class="p">,</span>
    <span class="n">RowParallelLinear</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">atom.models.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">IntermediateTensors</span><span class="p">,</span>
    <span class="n">PPMissingLayer</span><span class="p">,</span>
    <span class="n">make_empty_intermediate_tensors_factory</span><span class="p">,</span>
    <span class="n">make_layers</span><span class="p">,</span>
    <span class="n">maybe_prefix</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">atom.utils.decorators</span><span class="w"> </span><span class="kn">import</span> <span class="n">support_torch_compile</span>
</pre></div>
</div>
</section>
<section id="step-2-implement-layer-classes">
<h3>Step 2: Implement Layer Classes<a class="headerlink" href="#step-2-implement-layer-classes" title="Link to this heading"></a></h3>
<p>Each model typically defines three core module classes:</p>
<ol class="arabic simple">
<li><p><strong>Attention module</strong> (e.g., <code class="docutils literal notranslate"><span class="pre">MyModelAttention</span></code>):</p>
<ul class="simple">
<li><p>Initialize <code class="docutils literal notranslate"><span class="pre">QKVParallelLinear</span></code> for query/key/value.</p></li>
<li><p>Initialize <code class="docutils literal notranslate"><span class="pre">RowParallelLinear</span></code> for output projection.</p></li>
<li><p>Set up rotary embeddings via <code class="docutils literal notranslate"><span class="pre">aiter.rotary_embedding.get_rope()</span></code>.</p></li>
<li><p>Create <code class="docutils literal notranslate"><span class="pre">Attention</span></code> from <code class="docutils literal notranslate"><span class="pre">atom.model_ops.base_attention</span></code>.</p></li>
</ul>
</li>
<li><p><strong>MLP module</strong> (e.g., <code class="docutils literal notranslate"><span class="pre">MyModelMLP</span></code>):</p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">MergedColumnParallelLinear</span></code> for gate+up projections.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">RowParallelLinear</span></code> for down projection.</p></li>
<li><p>For MoE models, use <code class="docutils literal notranslate"><span class="pre">FusedMoE</span></code> from <code class="docutils literal notranslate"><span class="pre">atom.model_ops.moe</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Decoder layer</strong> (e.g., <code class="docutils literal notranslate"><span class="pre">MyModelDecoderLayer</span></code>):</p>
<ul class="simple">
<li><p>Combine attention + MLP with RMSNorm layers.</p></li>
<li><p>Implement the forward pass with residual connections.</p></li>
</ul>
</li>
</ol>
</section>
<section id="step-3-implement-the-model-and-causallm-classes">
<h3>Step 3: Implement the Model and CausalLM Classes<a class="headerlink" href="#step-3-implement-the-model-and-causallm-classes" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Backbone model</strong> (e.g., <code class="docutils literal notranslate"><span class="pre">MyModel</span></code>):</p>
<ul class="simple">
<li><p>Decorate with <code class="docutils literal notranslate"><span class="pre">&#64;support_torch_compile</span></code>.</p></li>
<li><p>Initialize <code class="docutils literal notranslate"><span class="pre">VocabParallelEmbedding</span></code>, decoder layers via <code class="docutils literal notranslate"><span class="pre">make_layers()</span></code>, and final <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code>.</p></li>
<li><p>Support pipeline parallelism with <code class="docutils literal notranslate"><span class="pre">PPMissingLayer</span></code> and <code class="docutils literal notranslate"><span class="pre">IntermediateTensors</span></code>.</p></li>
</ul>
</li>
<li><p><strong>CausalLM wrapper</strong> (e.g., <code class="docutils literal notranslate"><span class="pre">MyModelForCausalLM</span></code>):</p>
<ul class="simple">
<li><p>Define <code class="docutils literal notranslate"><span class="pre">packed_modules_mapping</span></code> to map checkpoint weight names to ATOM’s fused parameter names.</p></li>
<li><p>Initialize the backbone model and <code class="docutils literal notranslate"><span class="pre">ParallelLMHead</span></code>.</p></li>
<li><p>Implement <code class="docutils literal notranslate"><span class="pre">forward()</span></code> (returns hidden states) and <code class="docutils literal notranslate"><span class="pre">compute_logits()</span></code> (returns logits via <code class="docutils literal notranslate"><span class="pre">lm_head</span></code>).</p></li>
<li><p>If the model uses MoE, implement <code class="docutils literal notranslate"><span class="pre">get_expert_mapping()</span></code> returning <code class="docutils literal notranslate"><span class="pre">FusedMoE.make_expert_params_mapping(...)</span></code>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="step-4-register-the-model">
<h3>Step 4: Register the Model<a class="headerlink" href="#step-4-register-the-model" title="Link to this heading"></a></h3>
<p>Add an entry to <code class="docutils literal notranslate"><span class="pre">support_model_arch_dict</span></code> in <code class="docutils literal notranslate"><span class="pre">atom/model_engine/model_runner.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">support_model_arch_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="o">...</span>
    <span class="s2">&quot;MyModelForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;atom.models.my_model.MyModelForCausalLM&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The key must exactly match the <code class="docutils literal notranslate"><span class="pre">architectures</span></code> field in the HuggingFace model’s <code class="docutils literal notranslate"><span class="pre">config.json</span></code>.</p>
</section>
<section id="step-5-handle-weight-loading">
<h3>Step 5: Handle Weight Loading<a class="headerlink" href="#step-5-handle-weight-loading" title="Link to this heading"></a></h3>
<p>Ensure your <code class="docutils literal notranslate"><span class="pre">packed_modules_mapping</span></code> correctly maps all checkpoint weight names that differ from ATOM’s internal names. Common patterns:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Checkpoint Name</p></th>
<th class="head"><p>ATOM Parameter</p></th>
<th class="head"><p>Shard ID</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">q_proj</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">qkv_proj</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;q&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">k_proj</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">qkv_proj</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;k&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">v_proj</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">qkv_proj</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;v&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">gate_proj</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gate_up_proj</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">up_proj</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gate_up_proj</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
</tr>
</tbody>
</table>
<p>For MoE models, add <code class="docutils literal notranslate"><span class="pre">get_expert_mapping()</span></code> to delegate to <code class="docutils literal notranslate"><span class="pre">FusedMoE.make_expert_params_mapping()</span></code> with the correct gate/down/up projection names and expert count.</p>
<p>If the checkpoint uses non-standard weight names (like GPT-OSS), define a <code class="docutils literal notranslate"><span class="pre">weights_mapping</span></code> class attribute to rename them at load time.</p>
</section>
</section>
<hr class="docutils" />
<section id="model-specific-optimizations">
<h2>5. Model-Specific Optimizations<a class="headerlink" href="#model-specific-optimizations" title="Link to this heading"></a></h2>
<section id="llama-fused-rmsnorm-quant-and-silu-mul-quant">
<h3>Llama: Fused RMSNorm+Quant and SiLU+Mul+Quant<a class="headerlink" href="#llama-fused-rmsnorm-quant-and-silu-mul-quant" title="Link to this heading"></a></h3>
<p>Llama supports two AITER Triton fused kernel optimizations:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">ATOM_LLAMA_ENABLE_AITER_TRITON_FUSED_RMSNORM_QUANT</span></code></strong>: Fuses the RMSNorm normalization with FP8 or MXFP4 quantization in a single kernel call. Applied to both <code class="docutils literal notranslate"><span class="pre">input_layernorm</span></code> and <code class="docutils literal notranslate"><span class="pre">post_attention_layernorm</span></code>. Eliminates an extra read/write pass over the hidden states.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">ATOM_LLAMA_ENABLE_AITER_TRITON_FUSED_SILU_MUL_QUANT</span></code></strong>: Fuses the SiLU activation, element-wise multiply, and quantization in the MLP. The <code class="docutils literal notranslate"><span class="pre">SiluAndMul</span></code> module receives the <code class="docutils literal notranslate"><span class="pre">fused_quant=True</span></code> flag and the quant config, producing quantized output directly for the down projection.</p></li>
</ul>
<p>Both are controlled by environment variables and read from <code class="docutils literal notranslate"><span class="pre">atom.utils.envs</span></code>.</p>
</section>
<section id="deepseek-v2-v3-mla-fused-input-norm-qk-norm-fusion">
<h3>DeepSeek V2/V3: MLA + Fused Input Norm + QK Norm Fusion<a class="headerlink" href="#deepseek-v2-v3-mla-fused-input-norm-qk-norm-fusion" title="Link to this heading"></a></h3>
<p>DeepSeek models use Multi-head Latent Attention (MLA) with LoRA-compressed projections (<code class="docutils literal notranslate"><span class="pre">q_lora_rank</span></code>, <code class="docutils literal notranslate"><span class="pre">kv_lora_rank</span></code>). Several fusion optimizations are available:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_DS_INPUT_RMSNORM_QUANT_FUSION</span></code></strong>: Fuses the input RMSNorm with quantization. Implemented via <code class="docutils literal notranslate"><span class="pre">_fuse_rmsnorm_quant()</span></code> which dispatches to either <code class="docutils literal notranslate"><span class="pre">_fuse_rmsnorm_fp4_quant()</span></code> or <code class="docutils literal notranslate"><span class="pre">_fused_rms_fp8_group_quant()</span></code> based on the quant dtype. When enabled, the allreduce+RMSNorm fusion is disabled for <code class="docutils literal notranslate"><span class="pre">input_layernorm</span></code> but kept for <code class="docutils literal notranslate"><span class="pre">post_attention_layernorm</span></code>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_DS_QKNORM_QUANT_FUSION</span></code></strong>: Fuses the Q/K LoRA layernorm with quantization via <code class="docutils literal notranslate"><span class="pre">_fuse_qkv_a_proj_reduce_rmsnorm_quant_fp4()</span></code> or the FP8 variant, which performs the fused QKV-A projection, RMSNorm on Q and KV components, and quantization in a single fused operation.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_ALLREDUCE_RMSNORM_FUSION</span></code></strong>: Fuses tensor-parallel allreduce with RMSNorm.</p></li>
<li><p><strong>FP8 MQA logits</strong>: <code class="docutils literal notranslate"><span class="pre">fp8_mqa_logits</span></code> and <code class="docutils literal notranslate"><span class="pre">deepgemm_fp8_paged_mqa_logits</span></code> implement FP8-precision attention score computation for MLA decode.</p></li>
<li><p><strong>FP4 support</strong>: MXFP4 quantized GEMM kernels (<code class="docutils literal notranslate"><span class="pre">gemm_afp4wfp4_preshuffle</span></code>, <code class="docutils literal notranslate"><span class="pre">gemm_a16wfp4_preshuffle</span></code>) and FP4 block-scale BMM via <code class="docutils literal notranslate"><span class="pre">is_rocm_aiter_fp4bmm_enabled()</span></code>.</p></li>
</ul>
</section>
<section id="qwen3-moe-qk-norm-rope-cache-quant-fusion">
<h3>Qwen3-MoE: QK Norm + RoPE + Cache + Quant Fusion<a class="headerlink" href="#qwen3-moe-qk-norm-rope-cache-quant-fusion" title="Link to this heading"></a></h3>
<p>When <code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_QK_NORM_ROPE_CACHE_QUANT_FUSION</span></code> is enabled, the <code class="docutils literal notranslate"><span class="pre">Qwen3MoeAttention</span></code> module:</p>
<ol class="arabic simple">
<li><p>Precomputes a joint <code class="docutils literal notranslate"><span class="pre">cos_sin_cache</span></code> by concatenating cosine and sine RoPE caches.</p></li>
<li><p>Passes <code class="docutils literal notranslate"><span class="pre">q_norm</span></code> and <code class="docutils literal notranslate"><span class="pre">k_norm</span></code> directly to the <code class="docutils literal notranslate"><span class="pre">Attention</span></code> module.</p></li>
<li><p>The attention backend then fuses QK normalization, RoPE application, KV cache write, and optional quantization into a single kernel pass.</p></li>
</ol>
<p>Additionally, <code class="docutils literal notranslate"><span class="pre">ATOM_ENABLE_ALLREDUCE_RMSNORM_FUSION</span></code> fuses allreduce with RMSNorm for both attention output and MoE output, reducing communication overhead.</p>
</section>
<section id="mtp-deepseek-multi-token-prediction">
<h3>MTP: DeepSeek Multi-Token Prediction<a class="headerlink" href="#mtp-deepseek-multi-token-prediction" title="Link to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">DeepSeekMTP</span></code> model serves as a speculative draft model:</p>
<ul class="simple">
<li><p>Each <code class="docutils literal notranslate"><span class="pre">DeepSeekMultiTokenPredictorLayer</span></code> takes the previous hidden state and the next token’s embedding, normalizes both (<code class="docutils literal notranslate"><span class="pre">enorm</span></code>, <code class="docutils literal notranslate"><span class="pre">hnorm</span></code>), concatenates them, and passes through a linear projection (<code class="docutils literal notranslate"><span class="pre">eh_proj</span></code>) followed by a standard <code class="docutils literal notranslate"><span class="pre">DeepseekV2DecoderLayer</span></code>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">SharedHead</span></code> provides per-layer norm + LM head for logit computation.</p></li>
<li><p>For FP4 quantized main models, MTP blocks fall back to non-FP4 quantization config to maintain draft model accuracy.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="source-files">
<h2>Source Files<a class="headerlink" href="#source-files" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>File</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_engine/model_runner.py</span></code></p></td>
<td><p>Model registry (<code class="docutils literal notranslate"><span class="pre">support_model_arch_dict</span></code>) and <code class="docutils literal notranslate"><span class="pre">ModelRunner</span></code> class</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/models/llama.py</span></code></p></td>
<td><p>Llama model: <code class="docutils literal notranslate"><span class="pre">LlamaForCausalLM</span></code>, <code class="docutils literal notranslate"><span class="pre">LlamaModel</span></code>, <code class="docutils literal notranslate"><span class="pre">LlamaDecoderLayer</span></code>, <code class="docutils literal notranslate"><span class="pre">LlamaAttention</span></code>, <code class="docutils literal notranslate"><span class="pre">LlamaMLP</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/models/qwen3.py</span></code></p></td>
<td><p>Qwen3 model: <code class="docutils literal notranslate"><span class="pre">Qwen3ForCausalLM</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen3Model</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen3DecoderLayer</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen3Attention</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen3MLP</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/models/qwen3_moe.py</span></code></p></td>
<td><p>Qwen3-MoE model: <code class="docutils literal notranslate"><span class="pre">Qwen3MoeForCausalLM</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen3MoeModel</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen3MoeDecoderLayer</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen3MoeAttention</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen3MoeSparseMoeBlock</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen3MoeMLP</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/models/deepseek_v2.py</span></code></p></td>
<td><p>DeepSeek V2/V3 model: <code class="docutils literal notranslate"><span class="pre">DeepseekV2ForCausalLM</span></code>, <code class="docutils literal notranslate"><span class="pre">DeepseekV3ForCausalLM</span></code>, <code class="docutils literal notranslate"><span class="pre">DeepseekV2Model</span></code>, <code class="docutils literal notranslate"><span class="pre">DeepseekV2DecoderLayer</span></code>, <code class="docutils literal notranslate"><span class="pre">DeepseekV2MLAAttention</span></code>, <code class="docutils literal notranslate"><span class="pre">DeepseekV2MoE</span></code>, <code class="docutils literal notranslate"><span class="pre">DeepseekV2MLP</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/models/deepseek_mtp.py</span></code></p></td>
<td><p>DeepSeek MTP draft model: <code class="docutils literal notranslate"><span class="pre">DeepSeekMTP</span></code>, <code class="docutils literal notranslate"><span class="pre">DeepSeekMultiTokenPredictor</span></code>, <code class="docutils literal notranslate"><span class="pre">DeepSeekMultiTokenPredictorLayer</span></code>, <code class="docutils literal notranslate"><span class="pre">SharedHead</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/models/mixtral.py</span></code></p></td>
<td><p>Mixtral model: <code class="docutils literal notranslate"><span class="pre">MixtralForCausalLM</span></code>, <code class="docutils literal notranslate"><span class="pre">MixtralModel</span></code>, <code class="docutils literal notranslate"><span class="pre">MixtralDecoderLayer</span></code>, <code class="docutils literal notranslate"><span class="pre">MixtralAttention</span></code>, <code class="docutils literal notranslate"><span class="pre">MixtralMoE</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/models/gpt_oss.py</span></code></p></td>
<td><p>GPT-OSS model: <code class="docutils literal notranslate"><span class="pre">GptOssForCausalLM</span></code>, <code class="docutils literal notranslate"><span class="pre">GptOssModel</span></code>, <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code>, <code class="docutils literal notranslate"><span class="pre">OAIAttention</span></code>, <code class="docutils literal notranslate"><span class="pre">MLPBlock</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/models/glm4_moe.py</span></code></p></td>
<td><p>GLM4-MoE model: <code class="docutils literal notranslate"><span class="pre">Glm4MoeForCausalLM</span></code>, <code class="docutils literal notranslate"><span class="pre">Glm4MoeModel</span></code>, <code class="docutils literal notranslate"><span class="pre">Glm4MoeDecoderLayer</span></code>, <code class="docutils literal notranslate"><span class="pre">Glm4MoeAttention</span></code>, <code class="docutils literal notranslate"><span class="pre">Glm4MoE</span></code>, <code class="docutils literal notranslate"><span class="pre">Glm4MoeMLP</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/models/utils.py</span></code></p></td>
<td><p>Model utilities: <code class="docutils literal notranslate"><span class="pre">IntermediateTensors</span></code>, <code class="docutils literal notranslate"><span class="pre">PPMissingLayer</span></code>, <code class="docutils literal notranslate"><span class="pre">make_layers</span></code>, <code class="docutils literal notranslate"><span class="pre">maybe_prefix</span></code>, <code class="docutils literal notranslate"><span class="pre">extract_layer_index</span></code>, <code class="docutils literal notranslate"><span class="pre">should_ignore_layer</span></code>, <code class="docutils literal notranslate"><span class="pre">get_quant_config_for_layer</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_loader/loader.py</span></code></p></td>
<td><p>Weight loading: <code class="docutils literal notranslate"><span class="pre">load_model</span></code>, <code class="docutils literal notranslate"><span class="pre">safetensors_weights_iterator</span></code>, <code class="docutils literal notranslate"><span class="pre">default_weight_loader</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">atom/model_loader/weight_utils.py</span></code></p></td>
<td><p>Weight utilities: <code class="docutils literal notranslate"><span class="pre">download_weights_from_hf</span></code>, <code class="docutils literal notranslate"><span class="pre">set_weight_attrs</span></code>, <code class="docutils literal notranslate"><span class="pre">filter_duplicate_safetensors_files</span></code></p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="configuration_guide.html" class="btn btn-neutral float-left" title="ATOM Configuration Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="model_ops_guide.html" class="btn btn-neutral float-right" title="ATOM Model Operations Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, AMD.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>